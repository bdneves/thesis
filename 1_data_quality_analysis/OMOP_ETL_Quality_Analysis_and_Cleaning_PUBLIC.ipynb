{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OMOP ETL Data Quality Analysis and Cleaning Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a comprehensive data quality analysis and cleaning pipeline for OMOP Common Data Model (CDM) tables. The pipeline processes raw OMOP data, identifies quality issues, and produces a clean dataset suitable for research and analysis.\n",
    "\n",
    "**Key Features:**\n",
    "- Automated schema validation and type checking\n",
    "- Comprehensive data quality metrics calculation\n",
    "- Statistical anomaly detection\n",
    "- Foreign key validation\n",
    "- Date consistency checks\n",
    "- Duplicate detection and removal\n",
    "- Detailed quality reporting\n",
    "\n",
    "**OMOP Tables Processed:**\n",
    "- Person\n",
    "- Concept\n",
    "- Condition Occurrence\n",
    "- Drug Exposure\n",
    "- Procedure Occurrence\n",
    "- Visit Occurrence\n",
    "- Measurement\n",
    "- Observation Period\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries\n",
    "\n",
    "We begin by importing all required PySpark and Python libraries for data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    avg, col, min as spark_min, max as spark_max, count, when, to_date, to_timestamp, year, month,\n",
    "    concat_ws, lit, lag, collect_list, first, last, explode, sum as spark_sum,\n",
    "    date_add, datediff, coalesce, floor\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Dict\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import builtins  # To use built-in functions like round\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define SchemaManager Class\n",
    "\n",
    "The `SchemaManager` class contains the schema definitions for all OMOP CDM tables, including:\n",
    "- Primary keys\n",
    "- Foreign key relationships\n",
    "- Date/timestamp columns\n",
    "- Data type specifications (integer, float, string)\n",
    "\n",
    "This provides a single source of truth for table structure and enables automated validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaManager:\n",
    "    @staticmethod \n",
    "    def get_schema_info() -> dict:\n",
    "        \"\"\"Returns schema definitions for OMOP CDM tables.\"\"\"\n",
    "        return {\n",
    "            \"person\": {\n",
    "                \"key_cols\": [\"PERSON_ID\"],\n",
    "                \"foreign_keys\": {},\n",
    "                \"date_cols\": [\"DEATH_DATETIME\"],\n",
    "                \"timestamp_cols\": [\"DT_CREATED\", \"DT_MODIFIED\"],\n",
    "                \"int_cols\": [\n",
    "                    \"PERSON_ID\", \"YEAR_OF_BIRTH\", \"MONTH_OF_BIRTH\", \"DAY_OF_BIRTH\",\n",
    "                    \"RACE_CONCEPT_ID\", \"ETHNICITY_CONCEPT_ID\", \"LOCATION_ID\", \n",
    "                    \"PROVIDER_ID\", \"CARE_SITE_ID\"\n",
    "                ],\n",
    "                \"float_cols\": [],\n",
    "                \"string_cols\": [\n",
    "                    \"GENDER_CONCEPT_ID\", \"PERSON_SOURCE_VALUE\", \"GENDER_SOURCE_VALUE\",\n",
    "                    \"RACE_SOURCE_VALUE\", \"ETHNICITY_SOURCE_VALUE\", \"CREATED_BY\",\n",
    "                    \"MODIFIED_BY\", \"HASH_COLUMN\"\n",
    "                ]\n",
    "            },\n",
    "            \"concept\": {\n",
    "                \"key_cols\": [\"CONCEPT_ID\"],\n",
    "                \"foreign_keys\": {},\n",
    "                \"date_cols\": [\"VALID_START_DATE\", \"VALID_END_DATE\"],\n",
    "                \"timestamp_cols\": [\"DT_CREATED\", \"DT_MODIFIED\"],\n",
    "                \"int_cols\": [\"CONCEPT_CLASS_ID\"],\n",
    "                \"float_cols\": [],\n",
    "                \"string_cols\": [\n",
    "                    \"CONCEPT_ID\", \"CONCEPT_NAME\", \"DOMAIN_ID\", \"VOCABULARY_ID\",\n",
    "                    \"STANDARD_CONCEPT\", \"CONCEPT_CODE\", \"INVALID_REASON\",\n",
    "                    \"CREATED_BY\", \"MODIFIED_BY\", \"HASH_COLUMN\"\n",
    "                ]\n",
    "            },\n",
    "            \"condition_occurrence\": {\n",
    "                \"key_cols\": [\"CONDITION_OCCURRENCE_ID\"],\n",
    "                \"foreign_keys\": {\n",
    "                    \"PERSON_ID\": \"person\",\n",
    "                    \"VISIT_OCCURRENCE_ID\": \"visit_occurrence\"\n",
    "                },\n",
    "                \"date_cols\": [\"CONDITION_START_DATE\", \"CONDITION_END_DATE\"],\n",
    "                \"timestamp_cols\": [\"CONDITION_START_DATETIME\", \"CONDITION_END_DATETIME\", \"DT_CREATED\", \"DT_MODIFIED\"],\n",
    "                \"int_cols\": [\n",
    "                    \"CONDITION_OCCURRENCE_ID\", \"PERSON_ID\", \"VISIT_OCCURRENCE_ID\",\n",
    "                    \"CONDITION_TYPE_CONCEPT_ID\", \"PROVIDER_ID\",\n",
    "                    \"VISIT_DETAIL_ID\", \"CONDITION_STATUS_CONCEPT_ID\"\n",
    "                ],\n",
    "                \"float_cols\": [],\n",
    "                \"string_cols\": [\n",
    "                    \"CONDITION_CONCEPT_ID\", \"STOP_REASON\", \"CONDITION_SOURCE_CONCEPT_ID\",\n",
    "                    \"CONDITION_SOURCE_VALUE\", \"CONDITION_STATUS_SOURCE_VALUE\",\n",
    "                    \"CREATED_BY\", \"MODIFIED_BY\", \"HASH_COLUMN\"\n",
    "                ]\n",
    "            },\n",
    "            \"drug_exposure\": {\n",
    "                \"key_cols\": [\"DRUG_EXPOSURE_ID\"],\n",
    "                \"foreign_keys\": {\n",
    "                    \"PERSON_ID\": \"person\",\n",
    "                    \"VISIT_OCCURRENCE_ID\": \"visit_occurrence\"\n",
    "                },\n",
    "                \"date_cols\": [\n",
    "                    \"DRUG_EXPOSURE_START_DATE\", \"DRUG_EXPOSURE_END_DATE\", \"VERBATIM_END_DATE\"\n",
    "                ],\n",
    "                \"timestamp_cols\": [\n",
    "                    \"DRUG_EXPOSURE_START_DATETIME\", \"DRUG_EXPOSURE_END_DATETIME\",\n",
    "                    \"DT_CREATED\", \"DT_MODIFIED\"\n",
    "                ],\n",
    "                \"int_cols\": [\n",
    "                    \"DRUG_EXPOSURE_ID\", \"PERSON_ID\", \"VISIT_OCCURRENCE_ID\",\n",
    "                    \"DRUG_TYPE_CONCEPT_ID\", \"PROVIDER_ID\", \"VISIT_DETAIL_ID\",\n",
    "                    \"REFILLS\", \"DAYS_SUPPLY\", \"ROUTE_CONCEPT_ID\"\n",
    "                ],\n",
    "                \"float_cols\": [\n",
    "                    \"QUANTITY\"\n",
    "                ],\n",
    "                \"string_cols\": [\n",
    "                    \"DRUG_CONCEPT_ID\", \"STOP_REASON\", \"LOT_NUMBER\",\n",
    "                    \"SIG\", \"ROUTE_SOURCE_VALUE\", \"DOSE_UNIT_SOURCE_VALUE\",\n",
    "                    \"CREATED_BY\", \"MODIFIED_BY\", \"HASH_COLUMN\"\n",
    "                ]\n",
    "            },\n",
    "            \"procedure_occurrence\": {\n",
    "                \"key_cols\": [\"PROCEDURE_OCCURRENCE_ID\"],\n",
    "                \"foreign_keys\": {\n",
    "                    \"PERSON_ID\": \"person\",\n",
    "                    \"VISIT_OCCURRENCE_ID\": \"visit_occurrence\"\n",
    "                },\n",
    "                \"date_cols\": [\"PROCEDURE_DATE\"],\n",
    "                \"timestamp_cols\": [\"PROCEDURE_DATETIME\", \"DT_CREATED\", \"DT_MODIFIED\"],\n",
    "                \"int_cols\": [\n",
    "                    \"PROCEDURE_OCCURRENCE_ID\", \"PERSON_ID\", \"VISIT_OCCURRENCE_ID\",\n",
    "                    \"PROCEDURE_TYPE_CONCEPT_ID\", \"MODIFIER_CONCEPT_ID\",\n",
    "                    \"QUANTITY\", \"PROVIDER_ID\", \"VISIT_DETAIL_ID\"\n",
    "                ],\n",
    "                \"float_cols\": [],\n",
    "                \"string_cols\": [\n",
    "                    \"PROCEDURE_CONCEPT_ID\", \"PROCEDURE_SOURCE_CONCEPT_ID\",\n",
    "                    \"PROCEDURE_SOURCE_VALUE\", \"MODIFIER_SOURCE_VALUE\",\n",
    "                    \"CREATED_BY\", \"MODIFIED_BY\", \"HASH_COLUMN\"\n",
    "                ]\n",
    "            },\n",
    "            \"visit_occurrence\": {\n",
    "                \"key_cols\": [\"VISIT_OCCURRENCE_ID\"],\n",
    "                \"foreign_keys\": {\n",
    "                    \"PERSON_ID\": \"person\"\n",
    "                },\n",
    "                \"date_cols\": [\"VISIT_START_DATE\", \"VISIT_END_DATE\"],\n",
    "                \"timestamp_cols\": [\"VISIT_START_DATETIME\", \"VISIT_END_DATETIME\", \"DT_CREATED\", \"DT_MODIFIED\"],\n",
    "                \"int_cols\": [\n",
    "                    \"VISIT_OCCURRENCE_ID\", \"PERSON_ID\", \"VISIT_TYPE_CONCEPT_ID\",\n",
    "                    \"PROVIDER_ID\", \"CARE_SITE_ID\", \"ADMITTED_FROM_CONCEPT_ID\",\n",
    "                    \"DISCHARGED_TO_CONCEPT_ID\", \"PRECEDING_VISIT_OCCURRENCE_ID\"\n",
    "                ],\n",
    "                \"float_cols\": [],\n",
    "                \"string_cols\": [\n",
    "                    \"VISIT_CONCEPT_ID\", \"VISIT_SOURCE_VALUE\", \"VISIT_SOURCE_CONCEPT_ID\",\n",
    "                    \"ADMITTED_FROM_SOURCE_VALUE\", \"DISCHARGED_TO_SOURCE_VALUE\",\n",
    "                    \"CREATED_BY\", \"MODIFIED_BY\", \"HASH_COLUMN\"\n",
    "                ]\n",
    "            },\n",
    "            \"measurement\": {\n",
    "                \"key_cols\": [\"MEASUREMENT_ID\"],\n",
    "                \"foreign_keys\": {\n",
    "                    \"PERSON_ID\": \"person\",\n",
    "                    \"VISIT_OCCURRENCE_ID\": \"visit_occurrence\"\n",
    "                },\n",
    "                \"date_cols\": [\"MEASUREMENT_DATE\"],\n",
    "                \"timestamp_cols\": [\"MEASUREMENT_DATETIME\", \"DT_CREATED\", \"DT_MODIFIED\"],\n",
    "                \"int_cols\": [\n",
    "                    \"MEASUREMENT_ID\", \"PERSON_ID\", \"VISIT_OCCURRENCE_ID\",\n",
    "                    \"MEASUREMENT_TYPE_CONCEPT_ID\", \"OPERATOR_CONCEPT_ID\",\n",
    "                    \"VALUE_AS_CONCEPT_ID\", \"UNIT_CONCEPT_ID\", \"PROVIDER_ID\",\n",
    "                    \"VISIT_DETAIL_ID\", \"MEASUREMENT_EVENT_ID\"\n",
    "                ],\n",
    "                \"float_cols\": [\n",
    "                    \"VALUE_AS_NUMBER\", \"RANGE_LOW\", \"RANGE_HIGH\"\n",
    "                ],\n",
    "                \"string_cols\": [\n",
    "                    \"MEASUREMENT_CONCEPT_ID\", \"MEASUREMENT_SOURCE_VALUE\",\n",
    "                    \"MEASUREMENT_SOURCE_CONCEPT_ID\", \"UNIT_SOURCE_VALUE\",\n",
    "                    \"VALUE_SOURCE_VALUE\", \"MEAS_EVENT_FIELD_CONCEPT_ID\",\n",
    "                    \"CREATED_BY\", \"MODIFIED_BY\", \"HASH_COLUMN\"\n",
    "                ]\n",
    "            },\n",
    "            \"observation_period\": {\n",
    "                \"key_cols\": [\"OBSERVATION_PERIOD_ID\"],\n",
    "                \"foreign_keys\": {\n",
    "                    \"PERSON_ID\": \"person\"\n",
    "                },\n",
    "                \"date_cols\": [\"OBSERVATION_PERIOD_START_DATE\", \"OBSERVATION_PERIOD_END_DATE\"],\n",
    "                \"timestamp_cols\": [\"DT_CREATED\", \"DT_MODIFIED\"],\n",
    "                \"int_cols\": [\n",
    "                    \"OBSERVATION_PERIOD_ID\", \"PERSON_ID\", \"PERIOD_TYPE_CONCEPT_ID\"\n",
    "                ],\n",
    "                \"float_cols\": [],\n",
    "                \"string_cols\": [\n",
    "                    \"CREATED_BY\", \"MODIFIED_BY\", \"HASH_COLUMN\"\n",
    "                ]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define OMOPDataCleaner Class\n",
    "\n",
    "The `OMOPDataCleaner` class is the core of our data quality pipeline. It provides methods for:\n",
    "\n",
    "### Key Functionality:\n",
    "\n",
    "1. **Data Quality Metrics**: Calculate comprehensive quality statistics\n",
    "   - Null rates\n",
    "   - Uniqueness\n",
    "   - Completeness\n",
    "   - Statistical measures (mean, std dev, min, max)\n",
    "\n",
    "2. **Anomaly Detection**: Identify outliers and data quality issues\n",
    "   - Statistical outliers (±3 standard deviations)\n",
    "   - Invalid date ranges\n",
    "   - Missing required values\n",
    "\n",
    "3. **Validation**: Ensure data integrity\n",
    "   - Primary key uniqueness\n",
    "   - Foreign key referential integrity\n",
    "   - Date consistency checks\n",
    "\n",
    "4. **Cleaning Operations**: Remove or fix problematic records\n",
    "   - Duplicate removal\n",
    "   - Orphaned record handling\n",
    "   - Invalid date correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OMOPDataCleaner:\n",
    "    def __init__(self, spark: SparkSession, schema_info: dict):\n",
    "        \"\"\"Initialize the data cleaner with Spark session and schema information.\n",
    "        \n",
    "        Args:\n",
    "            spark: Active SparkSession\n",
    "            schema_info: Dictionary containing schema definitions for all tables\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.schema_info = schema_info\n",
    "        self.logger = self._setup_logger()\n",
    "        \n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        \"\"\"Set up logging configuration.\"\"\"\n",
    "        logger = logging.getLogger('OMOPDataCleaner')\n",
    "        logger.setLevel(logging.INFO)\n",
    "        return logger\n",
    "    \n",
    "    def calculate_quality_metrics(self, df: DataFrame, table_name: str) -> dict:\n",
    "        \"\"\"Calculate comprehensive data quality metrics for a given table.\n",
    "        \n",
    "        Metrics calculated:\n",
    "        - Total record count\n",
    "        - Null rates per column\n",
    "        - Unique value counts\n",
    "        - Statistical measures (for numeric columns)\n",
    "        - Completeness scores\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            table_name: Name of the OMOP table\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing quality metrics\n",
    "        \"\"\"\n",
    "        total_rows = df.count()\n",
    "        metrics = {\n",
    "            'table_name': table_name,\n",
    "            'total_rows': total_rows,\n",
    "            'column_metrics': {}\n",
    "        }\n",
    "        \n",
    "        for column in df.columns:\n",
    "            # Calculate null rate\n",
    "            null_count = df.filter(col(column).isNull()).count()\n",
    "            null_rate = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "            \n",
    "            # Calculate unique values\n",
    "            unique_count = df.select(column).distinct().count()\n",
    "            \n",
    "            column_metrics = {\n",
    "                'null_count': null_count,\n",
    "                'null_rate': builtins.round(null_rate, 2),\n",
    "                'unique_count': unique_count,\n",
    "                'completeness': builtins.round((1 - null_rate/100) * 100, 2)\n",
    "            }\n",
    "            \n",
    "            # Add statistics for numeric columns\n",
    "            if column in self.schema_info[table_name].get('int_cols', []) or \\\n",
    "               column in self.schema_info[table_name].get('float_cols', []):\n",
    "                stats = df.select(\n",
    "                    avg(col(column)).alias('mean'),\n",
    "                    spark_min(col(column)).alias('min'),\n",
    "                    spark_max(col(column)).alias('max')\n",
    "                ).collect()[0]\n",
    "                \n",
    "                column_metrics.update({\n",
    "                    'mean': builtins.round(float(stats['mean']), 2) if stats['mean'] else None,\n",
    "                    'min': stats['min'],\n",
    "                    'max': stats['max']\n",
    "                })\n",
    "            \n",
    "            metrics['column_metrics'][column] = column_metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def detect_anomalies(self, df: DataFrame, table_name: str) -> dict:\n",
    "        \"\"\"Detect anomalies and data quality issues in the dataset.\n",
    "        \n",
    "        Checks performed:\n",
    "        - Statistical outliers (values beyond 3 standard deviations)\n",
    "        - Invalid date ranges (end date before start date)\n",
    "        - Future dates\n",
    "        - Missing required foreign keys\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            table_name: Name of the OMOP table\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing anomaly counts and details\n",
    "        \"\"\"\n",
    "        anomalies = {\n",
    "            'table_name': table_name,\n",
    "            'statistical_outliers': {},\n",
    "            'date_inconsistencies': {},\n",
    "            'future_dates': {}\n",
    "        }\n",
    "        \n",
    "        # Check for statistical outliers in numeric columns\n",
    "        numeric_cols = self.schema_info[table_name].get('int_cols', []) + \\\n",
    "                      self.schema_info[table_name].get('float_cols', [])\n",
    "        \n",
    "        for column in numeric_cols:\n",
    "            if column in df.columns:\n",
    "                # Calculate mean and standard deviation\n",
    "                stats = df.select(\n",
    "                    avg(col(column)).alias('mean'),\n",
    "                    avg((col(column) - avg(col(column))) ** 2).alias('variance')\n",
    "                ).collect()[0]\n",
    "                \n",
    "                if stats['mean'] and stats['variance']:\n",
    "                    mean_val = stats['mean']\n",
    "                    std_dev = stats['variance'] ** 0.5\n",
    "                    \n",
    "                    # Count outliers (beyond 3 standard deviations)\n",
    "                    outlier_count = df.filter(\n",
    "                        (col(column) < mean_val - 3 * std_dev) | \n",
    "                        (col(column) > mean_val + 3 * std_dev)\n",
    "                    ).count()\n",
    "                    \n",
    "                    if outlier_count > 0:\n",
    "                        anomalies['statistical_outliers'][column] = outlier_count\n",
    "        \n",
    "        # Check for date inconsistencies\n",
    "        date_cols = self.schema_info[table_name].get('date_cols', [])\n",
    "        \n",
    "        # Check if end dates are before start dates\n",
    "        if 'CONDITION_START_DATE' in df.columns and 'CONDITION_END_DATE' in df.columns:\n",
    "            invalid_dates = df.filter(\n",
    "                col('CONDITION_END_DATE') < col('CONDITION_START_DATE')\n",
    "            ).count()\n",
    "            if invalid_dates > 0:\n",
    "                anomalies['date_inconsistencies']['condition_dates'] = invalid_dates\n",
    "        \n",
    "        # Similar checks for other date pairs\n",
    "        date_pairs = [\n",
    "            ('VISIT_START_DATE', 'VISIT_END_DATE'),\n",
    "            ('DRUG_EXPOSURE_START_DATE', 'DRUG_EXPOSURE_END_DATE'),\n",
    "            ('OBSERVATION_PERIOD_START_DATE', 'OBSERVATION_PERIOD_END_DATE')\n",
    "        ]\n",
    "        \n",
    "        for start_col, end_col in date_pairs:\n",
    "            if start_col in df.columns and end_col in df.columns:\n",
    "                invalid_count = df.filter(\n",
    "                    col(end_col).isNotNull() & \n",
    "                    col(start_col).isNotNull() &\n",
    "                    (col(end_col) < col(start_col))\n",
    "                ).count()\n",
    "                if invalid_count > 0:\n",
    "                    anomalies['date_inconsistencies'][f'{start_col}_{end_col}'] = invalid_count\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def validate_foreign_keys(self, df: DataFrame, table_name: str, \n",
    "                            reference_tables: dict) -> dict:\n",
    "        \"\"\"Validate foreign key relationships.\n",
    "        \n",
    "        Checks that all foreign key values exist in the referenced tables.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            table_name: Name of the current table\n",
    "            reference_tables: Dictionary of reference table DataFrames\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing validation results and orphaned record counts\n",
    "        \"\"\"\n",
    "        validation_results = {\n",
    "            'table_name': table_name,\n",
    "            'foreign_key_violations': {}\n",
    "        }\n",
    "        \n",
    "        foreign_keys = self.schema_info[table_name].get('foreign_keys', {})\n",
    "        \n",
    "        for fk_col, ref_table in foreign_keys.items():\n",
    "            if fk_col in df.columns and ref_table in reference_tables:\n",
    "                ref_df = reference_tables[ref_table]\n",
    "                ref_key = self.schema_info[ref_table]['key_cols'][0]\n",
    "                \n",
    "                # Find orphaned records (foreign keys with no matching reference)\n",
    "                orphaned = df.join(\n",
    "                    ref_df.select(col(ref_key).alias('ref_key')),\n",
    "                    df[fk_col] == col('ref_key'),\n",
    "                    'left_anti'\n",
    "                ).filter(col(fk_col).isNotNull()).count()\n",
    "                \n",
    "                if orphaned > 0:\n",
    "                    validation_results['foreign_key_violations'][fk_col] = {\n",
    "                        'reference_table': ref_table,\n",
    "                        'orphaned_count': orphaned\n",
    "                    }\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def remove_duplicates(self, df: DataFrame, table_name: str) -> DataFrame:\n",
    "        \"\"\"Remove duplicate records based on primary keys.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            table_name: Name of the OMOP table\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with duplicates removed\n",
    "        \"\"\"\n",
    "        key_cols = self.schema_info[table_name]['key_cols']\n",
    "        \n",
    "        initial_count = df.count()\n",
    "        \n",
    "        # Remove duplicates keeping the first occurrence\n",
    "        df_cleaned = df.dropDuplicates(key_cols)\n",
    "        \n",
    "        final_count = df_cleaned.count()\n",
    "        duplicates_removed = initial_count - final_count\n",
    "        \n",
    "        if duplicates_removed > 0:\n",
    "            self.logger.info(\n",
    "                f\"{table_name}: Removed {duplicates_removed} duplicate records \"\n",
    "                f\"({builtins.round(duplicates_removed/initial_count*100, 2)}%)\"\n",
    "            )\n",
    "        \n",
    "        return df_cleaned\n",
    "    \n",
    "    def clean_table(self, df: DataFrame, table_name: str, \n",
    "                   reference_tables: dict = None) -> DataFrame:\n",
    "        \"\"\"Execute the complete cleaning pipeline for a table.\n",
    "        \n",
    "        Steps:\n",
    "        1. Remove duplicates\n",
    "        2. Validate foreign keys (if reference tables provided)\n",
    "        3. Remove records with invalid foreign keys\n",
    "        4. Validate date consistency\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            table_name: Name of the OMOP table\n",
    "            reference_tables: Dictionary of reference DataFrames for FK validation\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting cleaning process for {table_name}\")\n",
    "        initial_count = df.count()\n",
    "        \n",
    "        # Step 1: Remove duplicates\n",
    "        df = self.remove_duplicates(df, table_name)\n",
    "        \n",
    "        # Step 2: Remove records with null primary keys\n",
    "        key_cols = self.schema_info[table_name]['key_cols']\n",
    "        for key_col in key_cols:\n",
    "            if key_col in df.columns:\n",
    "                df = df.filter(col(key_col).isNotNull())\n",
    "        \n",
    "        # Step 3: Validate and clean foreign keys\n",
    "        if reference_tables:\n",
    "            foreign_keys = self.schema_info[table_name].get('foreign_keys', {})\n",
    "            for fk_col, ref_table in foreign_keys.items():\n",
    "                if fk_col in df.columns and ref_table in reference_tables:\n",
    "                    ref_df = reference_tables[ref_table]\n",
    "                    ref_key = self.schema_info[ref_table]['key_cols'][0]\n",
    "                    \n",
    "                    # Keep only records with valid foreign keys or null foreign keys\n",
    "                    valid_fk_df = df.join(\n",
    "                        ref_df.select(col(ref_key).alias('ref_key')),\n",
    "                        df[fk_col] == col('ref_key'),\n",
    "                        'left_semi'\n",
    "                    )\n",
    "                    \n",
    "                    null_fk_df = df.filter(col(fk_col).isNull())\n",
    "                    df = valid_fk_df.union(null_fk_df)\n",
    "        \n",
    "        final_count = df.count()\n",
    "        records_removed = initial_count - final_count\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"{table_name}: Cleaning complete. \"\n",
    "            f\"Records: {initial_count} → {final_count} \"\n",
    "            f\"(removed {records_removed}, {builtins.round(records_removed/initial_count*100, 2)}%)\"\n",
    "        )\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Raw OMOP Data\n",
    "\n",
    "Load the raw OMOP CDM tables from your data source. The tables should be in Parquet format stored in a designated directory.\n",
    "\n",
    "**Note**: Replace the `base_path` with your actual data directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session (if not already initialized)\n",
    "# spark = SparkSession.builder.appName(\"OMOP_ETL_Cleaning\").getOrCreate()\n",
    "\n",
    "# Define base path for raw OMOP data\n",
    "# REPLACE WITH YOUR ACTUAL PATH\n",
    "raw_data_path = \"/path/to/your/raw/omop/data/\"\n",
    "\n",
    "# List of tables to process\n",
    "table_names = [\n",
    "    \"person\",\n",
    "    \"concept\",\n",
    "    \"condition_occurrence\",\n",
    "    \"drug_exposure\",\n",
    "    \"procedure_occurrence\",\n",
    "    \"visit_occurrence\",\n",
    "    \"measurement\",\n",
    "    \"observation_period\"\n",
    "]\n",
    "\n",
    "# Load all tables into a dictionary\n",
    "raw_tables = {}\n",
    "\n",
    "print(\"Loading raw OMOP tables...\\n\")\n",
    "for table_name in table_names:\n",
    "    try:\n",
    "        df = spark.read.parquet(f\"{raw_data_path}{table_name}\")\n",
    "        raw_tables[table_name] = df\n",
    "        print(f\"✓ Loaded '{table_name}' with {df.count():,} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading '{table_name}': {str(e)}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(raw_tables)} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Data Cleaner\n",
    "\n",
    "Create an instance of the `OMOPDataCleaner` with the schema information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get schema information\n",
    "schema_info = SchemaManager.get_schema_info()\n",
    "\n",
    "# Initialize the data cleaner\n",
    "cleaner = OMOPDataCleaner(spark, schema_info)\n",
    "\n",
    "print(\"Data cleaner initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Data Quality (Before Cleaning)\n",
    "\n",
    "Calculate quality metrics for all tables before cleaning to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating data quality metrics...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "quality_metrics_before = {}\n",
    "\n",
    "for table_name, df in raw_tables.items():\n",
    "    print(f\"\\nAnalyzing {table_name}...\")\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    metrics = cleaner.calculate_quality_metrics(df, table_name)\n",
    "    quality_metrics_before[table_name] = metrics\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  Total Records: {metrics['total_rows']:,}\")\n",
    "    \n",
    "    # Find columns with high null rates\n",
    "    high_null_cols = [\n",
    "        (col_name, col_metrics['null_rate']) \n",
    "        for col_name, col_metrics in metrics['column_metrics'].items()\n",
    "        if col_metrics['null_rate'] > 50\n",
    "    ]\n",
    "    \n",
    "    if high_null_cols:\n",
    "        print(f\"  Columns with >50% null values: {len(high_null_cols)}\")\n",
    "        for col_name, null_rate in sorted(high_null_cols, key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"    - {col_name}: {null_rate}%\")\n",
    "    \n",
    "    # Detect anomalies\n",
    "    anomalies = cleaner.detect_anomalies(df, table_name)\n",
    "    \n",
    "    if anomalies['statistical_outliers']:\n",
    "        print(f\"  Statistical outliers detected in {len(anomalies['statistical_outliers'])} columns\")\n",
    "    \n",
    "    if anomalies['date_inconsistencies']:\n",
    "        print(f\"  Date inconsistencies found: {sum(anomalies['date_inconsistencies'].values())} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Data quality analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean OMOP Tables\n",
    "\n",
    "Execute the cleaning pipeline on all tables. The cleaning process follows a specific order to maintain referential integrity:\n",
    "\n",
    "1. **Core tables** (no foreign keys): person, concept\n",
    "2. **Visit table**: visit_occurrence (references person)\n",
    "3. **Clinical event tables**: condition_occurrence, drug_exposure, procedure_occurrence, measurement (reference person and visit)\n",
    "4. **Observation period**: observation_period (references person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting data cleaning process...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cleaned_tables = {}\n",
    "\n",
    "# Phase 1: Clean core tables (no foreign keys)\n",
    "print(\"\\nPhase 1: Cleaning core tables...\")\n",
    "for table_name in ['person', 'concept']:\n",
    "    if table_name in raw_tables:\n",
    "        print(f\"\\nCleaning {table_name}...\")\n",
    "        cleaned_tables[table_name] = cleaner.clean_table(\n",
    "            raw_tables[table_name], \n",
    "            table_name\n",
    "        )\n",
    "\n",
    "# Phase 2: Clean tables with foreign key to person\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Phase 2: Cleaning tables with person reference...\")\n",
    "for table_name in ['visit_occurrence', 'observation_period']:\n",
    "    if table_name in raw_tables:\n",
    "        print(f\"\\nCleaning {table_name}...\")\n",
    "        cleaned_tables[table_name] = cleaner.clean_table(\n",
    "            raw_tables[table_name],\n",
    "            table_name,\n",
    "            reference_tables={'person': cleaned_tables['person']}\n",
    "        )\n",
    "\n",
    "# Phase 3: Clean clinical event tables (reference person and visit)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Phase 3: Cleaning clinical event tables...\")\n",
    "clinical_tables = ['condition_occurrence', 'drug_exposure', 'procedure_occurrence', 'measurement']\n",
    "\n",
    "reference_dict = {\n",
    "    'person': cleaned_tables['person'],\n",
    "    'visit_occurrence': cleaned_tables.get('visit_occurrence')\n",
    "}\n",
    "\n",
    "for table_name in clinical_tables:\n",
    "    if table_name in raw_tables:\n",
    "        print(f\"\\nCleaning {table_name}...\")\n",
    "        cleaned_tables[table_name] = cleaner.clean_table(\n",
    "            raw_tables[table_name],\n",
    "            table_name,\n",
    "            reference_tables=reference_dict\n",
    "        )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nCleaning complete! Processed {len(cleaned_tables)} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Data Quality (After Cleaning)\n",
    "\n",
    "Recalculate quality metrics to assess the impact of the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating post-cleaning quality metrics...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "quality_metrics_after = {}\n",
    "\n",
    "print(\"\\n{:<30} {:>15} {:>15} {:>15}\".format(\n",
    "    \"Table\", \"Before\", \"After\", \"Change (%)\"\n",
    "))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for table_name, df in cleaned_tables.items():\n",
    "    # Calculate metrics\n",
    "    metrics = cleaner.calculate_quality_metrics(df, table_name)\n",
    "    quality_metrics_after[table_name] = metrics\n",
    "    \n",
    "    # Compare before and after\n",
    "    before_count = quality_metrics_before[table_name]['total_rows']\n",
    "    after_count = metrics['total_rows']\n",
    "    change_pct = ((after_count - before_count) / before_count * 100) if before_count > 0 else 0\n",
    "    \n",
    "    print(\"{:<30} {:>15,} {:>15,} {:>14.2f}%\".format(\n",
    "        table_name,\n",
    "        before_count,\n",
    "        after_count,\n",
    "        change_pct\n",
    "    ))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validate Foreign Key Integrity\n",
    "\n",
    "After cleaning, verify that all foreign key relationships are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validating foreign key integrity...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for table_name, df in cleaned_tables.items():\n",
    "    if schema_info[table_name].get('foreign_keys'):\n",
    "        print(f\"\\nValidating {table_name}...\")\n",
    "        \n",
    "        results = cleaner.validate_foreign_keys(\n",
    "            df, \n",
    "            table_name, \n",
    "            cleaned_tables\n",
    "        )\n",
    "        validation_results[table_name] = results\n",
    "        \n",
    "        if results['foreign_key_violations']:\n",
    "            print(f\"  ✗ Foreign key violations found:\")\n",
    "            for fk, details in results['foreign_key_violations'].items():\n",
    "                print(f\"    - {fk} → {details['reference_table']}: \"\n",
    "                      f\"{details['orphaned_count']} orphaned records\")\n",
    "        else:\n",
    "            print(f\"  ✓ All foreign keys valid\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Foreign key validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Cleaned Data\n",
    "\n",
    "Save the cleaned tables to Parquet format for future use.\n",
    "\n",
    "**Note**: Replace the `output_path` with your desired output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path for cleaned data\n",
    "# REPLACE WITH YOUR DESIRED OUTPUT PATH\n",
    "output_path = \"/path/to/your/cleaned/omop/data/\"\n",
    "\n",
    "print(\"Exporting cleaned tables...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def export_to_parquet(df, table_name):\n",
    "    \"\"\"Export DataFrame to Parquet format.\"\"\"\n",
    "    if df is not None:\n",
    "        output_file = f\"{output_path}{table_name}\"\n",
    "        print(f\"Exporting '{table_name}' to {output_file}...\")\n",
    "        \n",
    "        df.write \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .parquet(output_file)\n",
    "        \n",
    "        print(f\"  ✓ Successfully exported '{table_name}' ({df.count():,} records)\\n\")\n",
    "    else:\n",
    "        print(f\"  ✗ DataFrame for '{table_name}' is None. Skipping.\\n\")\n",
    "\n",
    "# Export all cleaned tables\n",
    "for table_name, df in cleaned_tables.items():\n",
    "    export_to_parquet(df, table_name)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAll cleaned tables exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Summary Report\n",
    "\n",
    "Create a comprehensive summary of the cleaning process, including:\n",
    "- Record count changes\n",
    "- Data quality improvements\n",
    "- Issues identified and resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" \" * 20 + \"CLEANING SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_records_before = sum(m['total_rows'] for m in quality_metrics_before.values())\n",
    "total_records_after = sum(m['total_rows'] for m in quality_metrics_after.values())\n",
    "total_removed = total_records_before - total_records_after\n",
    "removal_percentage = (total_removed / total_records_before * 100) if total_records_before > 0 else 0\n",
    "\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"  Total records before cleaning: {total_records_before:,}\")\n",
    "print(f\"  Total records after cleaning:  {total_records_after:,}\")\n",
    "print(f\"  Records removed:               {total_removed:,} ({removal_percentage:.2f}%)\")\n",
    "print(f\"  Number of tables processed:    {len(cleaned_tables)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\nPer-Table Summary:\")\n",
    "print(f\"\\n{:<30} {:>12} {:>12} {:>12} {:>12}\".format(\n",
    "    \"Table\", \"Before\", \"After\", \"Removed\", \"% Removed\"\n",
    "))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for table_name in cleaned_tables.keys():\n",
    "    before = quality_metrics_before[table_name]['total_rows']\n",
    "    after = quality_metrics_after[table_name]['total_rows']\n",
    "    removed = before - after\n",
    "    pct_removed = (removed / before * 100) if before > 0 else 0\n",
    "    \n",
    "    print(f\"{table_name:<30} {before:>12,} {after:>12,} {removed:>12,} {pct_removed:>11.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nData Cleaning Pipeline Complete!\")\n",
    "print(\"\\nCleaned dataset is ready for analysis and research.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Optional: Load and Verify Cleaned Data\n",
    "\n",
    "As a final verification step, you can reload the cleaned data and perform spot checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load and verify cleaned data\n",
    "\n",
    "# print(\"Loading cleaned tables for verification...\\n\")\n",
    "\n",
    "# loaded_tables = {}\n",
    "\n",
    "# for table_name in table_names:\n",
    "#     try:\n",
    "#         df = spark.read.parquet(f\"{output_path}{table_name}\")\n",
    "#         loaded_tables[table_name] = df\n",
    "#         print(f\"✓ Loaded '{table_name}' with {df.count():,} records\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Error loading '{table_name}': {str(e)}\")\n",
    "\n",
    "# # Example: Display sample from person table\n",
    "# if 'person' in loaded_tables:\n",
    "#     print(\"\\nSample from person table:\")\n",
    "#     loaded_tables['person'].show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes and Recommendations\n",
    "\n",
    "### Cleaning Strategy\n",
    "The cleaning pipeline follows these key principles:\n",
    "\n",
    "1. **Preserve Data Integrity**: Foreign key relationships are maintained throughout the cleaning process\n",
    "2. **Order of Operations**: Tables are cleaned in dependency order to prevent orphaned records\n",
    "3. **Conservative Approach**: Only clearly invalid data is removed (duplicates, null primary keys, orphaned foreign keys)\n",
    "\n",
    "### Data Quality Considerations\n",
    "\n",
    "**What Was Removed:**\n",
    "- Duplicate records (based on primary keys)\n",
    "- Records with null primary keys\n",
    "- Orphaned records (foreign keys pointing to non-existent records)\n",
    "\n",
    "**What Was Preserved:**\n",
    "- Records with high null rates in optional fields\n",
    "- Statistical outliers (flagged but not removed)\n",
    "- Records with date inconsistencies (flagged but not removed)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
