{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models for Chronic Condition Classification\n",
    "## Zero-shot Learning for Clinical Phenotyping\n",
    "\n",
    "This notebook documents the methodology used in the paper:\n",
    "\n",
    "**Zero-shot learning for clinical phenotyping: Comparing LLMs and rule-based methods**  \n",
    "*Computers in Biology and Medicine*  \n",
    "https://doi.org/10.1016/j.compbiomed.2025.110181\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment Overview\n",
    "\n",
    "**Objective**: Compare the performance of different approaches in classifying 20 chronic conditions from patient clinical notes using zero-shot learning.\n",
    "\n",
    "**Methods Evaluated**:\n",
    "1. **Rule-based dictionary approach** (baseline)\n",
    "2. **Large Language Models** via API calls\n",
    "\n",
    "**Dataset**: 1,000 patients with aggregated clinical notes\n",
    "- 57.5% female, 42.5% male\n",
    "- Mean age: 51.3 years\n",
    "\n",
    "**Chronic Conditions** (Top 20 by prevalence):\n",
    "1. Hyperlipidemia\n",
    "2. Autoimmune diseases\n",
    "3. Hypertension\n",
    "4. Benign prostate hypertrophy\n",
    "5. Cancer\n",
    "6. Heart Failure\n",
    "7. Arrhythmias\n",
    "8. Anxiety disorder\n",
    "9. Disorders of thyroid gland\n",
    "10. Ischemic heart disease\n",
    "11. Osteoporosis\n",
    "12. Disorder of vertebral column\n",
    "13. Depression\n",
    "14. Urinary incontinence\n",
    "15. Osteoarthritis\n",
    "16. Diabetes\n",
    "17. Asthma\n",
    "18. Anemia\n",
    "19. Peripheral neuropathy\n",
    "20. Epilepsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "### 1.1 Ground Truth Generation\n",
    "\n",
    "Ground truth labels were created using multiple sources:\n",
    "- **Diagnosis codes** (ICD-10)\n",
    "- **Medication prescriptions** (drug-condition mappings)\n",
    "- **Laboratory results** (condition-specific tests)\n",
    "- **Procedures** (condition-related procedures)\n",
    "\n",
    "Patient dictionaries were created mapping each chronic condition to patient IDs who have evidence of that condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading ground truth dictionaries\n",
    "import pickle\n",
    "\n",
    "# Load dictionaries mapping conditions to patient IDs\n",
    "# These were created from EHR data using rule-based classification\n",
    "\n",
    "# patients_by_condition.pkl: Based on diagnosis codes only\n",
    "with open('data/patients_by_condition.pkl', 'rb') as f:\n",
    "    patients_by_condition = pickle.load(f)\n",
    "\n",
    "# patients_by_drug_condition.pkl: Based on medication prescriptions\n",
    "with open('data/patients_by_drug_condition.pkl', 'rb') as f:\n",
    "    patients_by_drug_condition = pickle.load(f)\n",
    "\n",
    "# patients_by_lab.pkl: Based on laboratory results\n",
    "with open('data/patients_by_lab.pkl', 'rb') as f:\n",
    "    patients_by_lab = pickle.load(f)\n",
    "\n",
    "# patients_by_procedure.pkl: Based on procedures\n",
    "with open('data/patients_by_procedure.pkl', 'rb') as f:\n",
    "    patients_by_procedure = pickle.load(f)\n",
    "\n",
    "# patients_any.pkl: Union of all sources (comprehensive ground truth)\n",
    "with open('data/patients_any.pkl', 'rb') as f:\n",
    "    patients_any = pickle.load(f)\n",
    "\n",
    "print(f\"Total chronic conditions tracked: {len(patients_any)}\")\n",
    "print(f\"Available conditions: {list(patients_any.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Patient Clinical Note Aggregation\n",
    "\n",
    "For each patient, clinical data was aggregated into a structured JSON format containing:\n",
    "- **Demographics**: Age, gender\n",
    "- **Events by age**: Chronologically ordered clinical events\n",
    "- **Diagnoses**: All recorded diagnosis codes\n",
    "- **Medications**: Prescribed medications\n",
    "- **Laboratory results**: Key lab measurements\n",
    "- **Procedures**: Medical procedures performed\n",
    "\n",
    "This structured data serves as the input for LLM evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example of patient data structure\n",
    "example_patient = {\n",
    "    \"sex\": \"female\",\n",
    "    \"events_by_age\": [\n",
    "        {\n",
    "            \"age\": 45,\n",
    "            \"diagnoses\": [\"E11.9 - Type 2 diabetes mellitus without complications\"],\n",
    "            \"medications\": [\"Metformin 500mg\"],\n",
    "            \"lab_results\": [\"HbA1c: 7.2%\"],\n",
    "            \"procedures\": []\n",
    "        },\n",
    "        {\n",
    "            \"age\": 46,\n",
    "            \"diagnoses\": [\"E11.9 - Type 2 diabetes mellitus without complications\"],\n",
    "            \"medications\": [\"Metformin 500mg\", \"Lisinopril 10mg\"],\n",
    "            \"lab_results\": [\"HbA1c: 6.8%\", \"Blood pressure: 145/92\"],\n",
    "            \"procedures\": []\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# In practice, load from your data file\n",
    "# with open('data/data_all.json', 'r') as f:\n",
    "#     patient_data = json.load(f)\n",
    "\n",
    "print(\"Example patient data structure:\")\n",
    "print(json.dumps(example_patient, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: LLM Evaluation Framework\n",
    "\n",
    "### 2.1 Prompt Engineering\n",
    "\n",
    "A carefully designed prompt guides the LLM to:\n",
    "1. Act as an experienced doctor\n",
    "2. Review the patient's clinical note\n",
    "3. Assess presence of specific chronic conditions\n",
    "4. Consider:\n",
    "   - Direct mentions in diagnoses\n",
    "   - Related medical terms and subtypes\n",
    "   - Indirect evidence (medications, labs, procedures)\n",
    "5. Return structured JSON output with:\n",
    "   - Comorbidity name\n",
    "   - Rationale for decision\n",
    "   - Binary classification (true/false)\n",
    "   - Confidence level (low/medium/high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(patient_summary, chronic_condition):\n",
    "    \"\"\"\n",
    "    Create a structured prompt for LLM chronic condition assessment.\n",
    "    \n",
    "    Args:\n",
    "        patient_summary: JSON string of patient clinical data\n",
    "        chronic_condition: Name of condition to assess\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "# Task\n",
    "You are an experienced doctor tasked with determining if the patient has the chronic condition listed below based on their clinical note.\n",
    "\n",
    "# Patient\n",
    "Below is a clinical note describing the patient's aggregated health information:\n",
    "{patient_summary}\n",
    "\n",
    "# Chronic Condition\n",
    "The chronic condition being assessed is:\n",
    "{chronic_condition}\n",
    "\n",
    "# Assessment Instructions\n",
    "Evaluate only the chronic condition listed above. Do not consider or mention any other conditions.\n",
    "\n",
    "Use the patient's clinical note to determine whether the patient has that chronic condition. Provide a detailed explanation.\n",
    "\n",
    "First, consider any related medical terms, subtypes, or diagnoses related to the chronic condition. If any related terms or subtypes are found, confirm the presence of the condition.\n",
    "Then, look at the clinical note sections such as procedures, measurements, and medications to infer if there could be a chronic condition based on the procedures, laboratory results, and medications.\n",
    "Remember to consider subtypes of the chronic conditions when making your assessment.\n",
    "\n",
    "Provide your response as a JSON dictionary with the following elements:\n",
    "* comorbidity: str - The name of the comorbidity being assessed\n",
    "* rationale: str - Your reasoning for the assessment\n",
    "* is_met: bool - \"true\" if the patient has the comorbidity, otherwise \"false\"\n",
    "* confidence: str - Your confidence level (\"low\", \"medium\", \"high\")\n",
    "\n",
    "An example of how your JSON response should be formatted is shown below:\n",
    "```json\n",
    "{{\n",
    "    \"comorbidity\": \"{chronic_condition}\",\n",
    "    \"rationale\": \"Reason for assessment\",\n",
    "    \"is_met\": true/false,\n",
    "    \"confidence\": \"low/medium/high\"\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Example usage\n",
    "example_condition = \"Diabetes\"\n",
    "example_prompt = create_prompt(json.dumps(example_patient, indent=2), example_condition)\n",
    "print(\"Example prompt (truncated):\")\n",
    "print(example_prompt[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generic LLM API Client\n",
    "\n",
    "A generic client class that can interface with any LLM API endpoint (e.g., OpenAI, Azure OpenAI, Anthropic, local models, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class GenericLLMClient:\n",
    "    \"\"\"\n",
    "    Generic client for LLM API interactions.\n",
    "    Supports any API that accepts chat-style requests.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env_path='.env'):\n",
    "        \"\"\"\n",
    "        Initialize LLM client.\n",
    "        \n",
    "        Args:\n",
    "            env_path: Path to .env file containing API credentials\n",
    "        \"\"\"\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        \n",
    "        # Generic configuration - adapt based on your API\n",
    "        self.api_key = os.getenv(\"LLM_API_KEY\")\n",
    "        self.api_endpoint = os.getenv(\"LLM_API_ENDPOINT\")\n",
    "        self.api_version = os.getenv(\"LLM_API_VERSION\", \"\")\n",
    "        \n",
    "        self.validate_env_vars()\n",
    "    \n",
    "    def validate_env_vars(self):\n",
    "        \"\"\"Validate required environment variables.\"\"\"\n",
    "        if not all([self.api_key, self.api_endpoint]):\n",
    "            raise ValueError(\"Missing required API credentials in .env file\")\n",
    "    \n",
    "    def get_llm_response(self, model_name, prompt_text, temperature=0.0):\n",
    "        \"\"\"\n",
    "        Get LLM response for a given prompt.\n",
    "        \n",
    "        This is a generic implementation. Adapt the request format\n",
    "        based on your specific API requirements.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Model identifier\n",
    "            prompt_text: The prompt to send\n",
    "            temperature: Sampling temperature (0.0 for deterministic)\n",
    "        \n",
    "        Returns:\n",
    "            Response text from the model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generic request structure - adapt for your API\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": model_name,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an experienced doctor evaluating a patient's medical record.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt_text\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                self.api_endpoint,\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=120\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse response - adapt based on API response format\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in getting LLM response: {e}\")\n",
    "            return None\n",
    "\n",
    "print(\"Generic LLM client class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Response Processing\n",
    "\n",
    "LLM responses need to be parsed and validated to extract structured predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_response(response):\n",
    "    \"\"\"\n",
    "    Clean JSON response from LLM output.\n",
    "    \n",
    "    Removes markdown code fences and other formatting.\n",
    "    \n",
    "    Args:\n",
    "        response: Raw response string from LLM\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned JSON string\n",
    "    \"\"\"\n",
    "    # Remove markdown JSON code blocks\n",
    "    if response.startswith('```json'):\n",
    "        response = response[7:]\n",
    "    if response.endswith('```'):\n",
    "        response = response[:-3]\n",
    "    return response.strip()\n",
    "\n",
    "def evaluate_condition(client, model_name, patient_id, patient_summary, chronic_condition):\n",
    "    \"\"\"\n",
    "    Evaluate a single chronic condition for a patient.\n",
    "    \n",
    "    Args:\n",
    "        client: LLM client (Azure or Ollama)\n",
    "        model_name: Model identifier\n",
    "        patient_id: Patient identifier\n",
    "        patient_summary: JSON string of patient data\n",
    "        chronic_condition: Condition name to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with assessment results or None if error\n",
    "    \"\"\"\n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(patient_summary, chronic_condition)\n",
    "    logging.info(f\"Evaluating patient {patient_id} for {chronic_condition}\")\n",
    "    \n",
    "    # Get the response from the model\n",
    "    response = client.get_llm_response(model_name, prompt)\n",
    "    \n",
    "    # Parse and validate the response\n",
    "    if response:\n",
    "        try:\n",
    "            cleaned_response = clean_json_response(response)\n",
    "            assessment = json.loads(cleaned_response)\n",
    "            \n",
    "            # Validate response structure\n",
    "            required_fields = ['comorbidity', 'rationale', 'is_met', 'confidence']\n",
    "            if not all(field in assessment for field in required_fields):\n",
    "                raise ValueError(\"Missing required fields in response\")\n",
    "            \n",
    "            # Validate condition matches\n",
    "            if assessment[\"comorbidity\"] != chronic_condition:\n",
    "                raise ValueError(\"Response comorbidity does not match requested condition\")\n",
    "            \n",
    "            return assessment\n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            logging.error(f\"Error parsing response for patient {patient_id}, \"\n",
    "                         f\"condition {chronic_condition}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(\"Response processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Running the Experiment\n",
    "\n",
    "### 3.1 Main Evaluation Loop\n",
    "\n",
    "The evaluation process:\n",
    "1. Loads patient data\n",
    "2. For each patient:\n",
    "   - For each chronic condition:\n",
    "     - Generate prompt\n",
    "     - Get LLM prediction\n",
    "     - Parse and store result\n",
    "3. Save results incrementally (to handle interruptions)\n",
    "4. Resume from last checkpoint if interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, output_file):\n",
    "    \"\"\"Save evaluation results to JSON file.\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    logging.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "def run_evaluation(client, model_name, patient_data_file, chronic_conditions_list, \n",
    "                   output_file, evaluation_type='single'):\n",
    "    \"\"\"\n",
    "    Run chronic condition evaluation for all patients.\n",
    "    \n",
    "    Args:\n",
    "        client: LLM client instance\n",
    "        model_name: Model identifier\n",
    "        patient_data_file: Path to patient data JSON\n",
    "        chronic_conditions_list: List of conditions to evaluate\n",
    "        output_file: Path to save results\n",
    "        evaluation_type: 'single' (one condition at a time) or 'batch' (all conditions)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Load existing results if available (for resuming)\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        logging.info(f\"Loaded existing results from {output_file}\")\n",
    "    \n",
    "    # Load patient data\n",
    "    with open(patient_data_file, 'r') as f:\n",
    "        patient_data = json.load(f)\n",
    "    \n",
    "    # Evaluate each patient\n",
    "    for patient_id, patient_info in patient_data.items():\n",
    "        patient_summary = json.dumps(patient_info)\n",
    "        logging.info(f\"Evaluating patient ID: {patient_id}\")\n",
    "        \n",
    "        # Initialize patient results if not exists\n",
    "        if patient_id not in results:\n",
    "            results[patient_id] = {\n",
    "                \"patient_summary\": patient_summary,\n",
    "                \"assessments\": []\n",
    "            }\n",
    "        \n",
    "        # Determine which conditions still need evaluation\n",
    "        existing_conditions = {\n",
    "            assessment['comorbidity'] \n",
    "            for assessment in results[patient_id]['assessments']\n",
    "        }\n",
    "        remaining_conditions = [\n",
    "            condition \n",
    "            for condition in chronic_conditions_list \n",
    "            if condition not in existing_conditions\n",
    "        ]\n",
    "        \n",
    "        # Skip if patient fully evaluated\n",
    "        if not remaining_conditions:\n",
    "            logging.info(f\"Patient {patient_id} already fully evaluated\")\n",
    "            continue\n",
    "        \n",
    "        # Evaluate remaining conditions\n",
    "        for condition in remaining_conditions:\n",
    "            assessment = evaluate_condition(\n",
    "                client, model_name, patient_id, patient_summary, condition\n",
    "            )\n",
    "            \n",
    "            if assessment:\n",
    "                results[patient_id]['assessments'].append(assessment)\n",
    "                # Save progress after each condition\n",
    "                save_results(results, output_file)\n",
    "    \n",
    "    logging.info(f\"Evaluation complete. Results saved to {output_file}\")\n",
    "    return results\n",
    "\n",
    "print(\"Evaluation loop defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Example: Running Evaluation\n",
    "\n",
    "Example of running the evaluation with an LLM API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHRONIC_CONDITIONS = [\n",
    "    'Hyperlipidemia', 'Autoimmune diseases', 'Hypertension', \n",
    "    'Benign prostate hypertrophy', 'Cancer', 'Heart Failure', \n",
    "    'Arrhythmias', 'Anxiety disorder', 'Disorders of thyroid gland', \n",
    "    'Ischemic heart disease', 'Osteoporosis', 'Disorder of vertebral column', \n",
    "    'Depression', 'Urinary incontinence', 'Osteoarthritis', 'Diabetes', \n",
    "    'Asthma', 'Anemia', 'Peripheral neuropathy', 'Epilepsy'\n",
    "]\n",
    "\n",
    "# Example: Run evaluation (requires actual data and API credentials)\n",
    "\"\"\"\n",
    "# Initialize LLM client\n",
    "client = GenericLLMClient(env_path='.env')\n",
    "\n",
    "# Run evaluation\n",
    "results = run_evaluation(\n",
    "    client=client,\n",
    "    model_name='your-model-name',  # Specify your model\n",
    "    patient_data_file='data/patient_data.json',\n",
    "    chronic_conditions_list=CHRONIC_CONDITIONS,\n",
    "    output_file='results/model_results.json',\n",
    "    evaluation_type='single'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example evaluation code provided (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Evaluation Metrics\n",
    "\n",
    "### 4.1 Computing Performance Metrics\n",
    "\n",
    "We compare LLM predictions against ground truth labels using standard classification metrics:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: Of predicted positives, how many are correct\n",
    "- **Recall (Sensitivity)**: Of actual positives, how many are detected\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **Specificity**: Of actual negatives, how many are correctly identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "def calculate_metrics(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth: List of true labels (0/1)\n",
    "        predictions: List of predicted labels (0/1)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(ground_truth, predictions).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(ground_truth, predictions),\n",
    "        'precision': precision_score(ground_truth, predictions, zero_division=0),\n",
    "        'recall': recall_score(ground_truth, predictions, zero_division=0),\n",
    "        'f1_score': f1_score(ground_truth, predictions, zero_division=0),\n",
    "        'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'true_negatives': tn,\n",
    "        'false_negatives': fn\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_model_performance(results_file, ground_truth_dict, conditions_list):\n",
    "    \"\"\"\n",
    "    Evaluate model performance against ground truth.\n",
    "    \n",
    "    Args:\n",
    "        results_file: Path to model results JSON\n",
    "        ground_truth_dict: Dictionary mapping conditions to patient IDs\n",
    "        conditions_list: List of conditions to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with metrics per condition\n",
    "    \"\"\"\n",
    "    # Load results\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Calculate metrics for each condition\n",
    "    condition_metrics = []\n",
    "    \n",
    "    for condition in conditions_list:\n",
    "        ground_truth = []\n",
    "        predictions = []\n",
    "        \n",
    "        # Collect ground truth and predictions for this condition\n",
    "        for patient_id, patient_results in results.items():\n",
    "            # Ground truth: is patient in ground truth set?\n",
    "            gt_label = 1 if int(patient_id) in ground_truth_dict.get(condition, []) else 0\n",
    "            \n",
    "            # Find prediction for this condition\n",
    "            pred_label = 0\n",
    "            for assessment in patient_results['assessments']:\n",
    "                if assessment['comorbidity'] == condition:\n",
    "                    pred_label = 1 if assessment['is_met'] else 0\n",
    "                    break\n",
    "            \n",
    "            ground_truth.append(gt_label)\n",
    "            predictions.append(pred_label)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(ground_truth, predictions)\n",
    "        metrics['condition'] = condition\n",
    "        metrics['prevalence'] = sum(ground_truth) / len(ground_truth)\n",
    "        condition_metrics.append(metrics)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_metrics = pd.DataFrame(condition_metrics)\n",
    "    \n",
    "    # Reorder columns\n",
    "    column_order = [\n",
    "        'condition', 'prevalence', 'accuracy', 'precision', \n",
    "        'recall', 'f1_score', 'specificity',\n",
    "        'true_positives', 'false_positives', \n",
    "        'true_negatives', 'false_negatives'\n",
    "    ]\n",
    "    df_metrics = df_metrics[column_order]\n",
    "    \n",
    "    return df_metrics\n",
    "\n",
    "print(\"Metrics calculation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Example: Computing Metrics\n",
    "\n",
    "Example of computing metrics for a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute metrics (requires actual results)\n",
    "\"\"\"\n",
    "# Load ground truth\n",
    "with open('data/patients_any.pkl', 'rb') as f:\n",
    "    ground_truth = pickle.load(f)\n",
    "\n",
    "# Evaluate model performance\n",
    "model_metrics = evaluate_model_performance(\n",
    "    results_file='results/model_results.json',\n",
    "    ground_truth_dict=ground_truth,\n",
    "    conditions_list=CHRONIC_CONDITIONS\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(model_metrics.round(3))\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_metrics = {\n",
    "    'Mean Accuracy': model_metrics['accuracy'].mean(),\n",
    "    'Mean Precision': model_metrics['precision'].mean(),\n",
    "    'Mean Recall': model_metrics['recall'].mean(),\n",
    "    'Mean F1 Score': model_metrics['f1_score'].mean(),\n",
    "    'Mean Specificity': model_metrics['specificity'].mean()\n",
    "}\n",
    "\n",
    "print(\"\\nOverall Performance:\")\n",
    "for metric, value in overall_metrics.items():\n",
    "    print(f\"{metric}: {value:.3f}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example metrics computation provided (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comparing Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def compare_models(results_files, model_names, ground_truth_dict, conditions_list):\n",
    "    \"\"\"\n",
    "    Compare performance across multiple models.\n",
    "    \n",
    "    Args:\n",
    "        results_files: List of paths to model results\n",
    "        model_names: List of model names\n",
    "        ground_truth_dict: Ground truth dictionary\n",
    "        conditions_list: List of conditions\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with comparison\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    \n",
    "    for results_file, model_name in zip(results_files, model_names):\n",
    "        metrics = evaluate_model_performance(\n",
    "            results_file, ground_truth_dict, conditions_list\n",
    "        )\n",
    "        metrics['model'] = model_name\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    # Combine all metrics\n",
    "    df_comparison = pd.concat(all_metrics, ignore_index=True)\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "def plot_model_comparison(df_comparison, metric='f1_score'):\n",
    "    \"\"\"\n",
    "    Create visualization comparing models.\n",
    "    \n",
    "    Args:\n",
    "        df_comparison: DataFrame from compare_models\n",
    "        metric: Which metric to plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Boxplot comparing models\n",
    "    sns.boxplot(data=df_comparison, x='model', y=metric)\n",
    "    plt.title(f'Model Comparison: {metric.replace(\"_\", \" \").title()}')\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{metric.upper()} Summary by Model:\")\n",
    "    summary = df_comparison.groupby('model')[metric].agg(['mean', 'std', 'min', 'max'])\n",
    "    print(summary.round(3))\n",
    "\n",
    "print(\"Model comparison functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Analysis and Insights\n",
    "\n",
    "### 5.1 Condition-Specific Performance\n",
    "\n",
    "Analyze which conditions are easier or harder to detect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_condition_difficulty(df_metrics, top_n=10):\n",
    "    \"\"\"\n",
    "    Identify easiest and hardest conditions to detect.\n",
    "    \n",
    "    Args:\n",
    "        df_metrics: DataFrame with condition metrics\n",
    "        top_n: Number of conditions to show\n",
    "    \"\"\"\n",
    "    # Sort by F1 score\n",
    "    df_sorted = df_metrics.sort_values('f1_score', ascending=False)\n",
    "    \n",
    "    print(f\"Top {top_n} Easiest Conditions to Detect:\")\n",
    "    print(df_sorted[['condition', 'f1_score', 'precision', 'recall']].head(top_n))\n",
    "    \n",
    "    print(f\"\\nTop {top_n} Hardest Conditions to Detect:\")\n",
    "    print(df_sorted[['condition', 'f1_score', 'precision', 'recall']].tail(top_n))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot F1 scores\n",
    "    plt.barh(range(len(df_sorted)), df_sorted['f1_score'])\n",
    "    plt.yticks(range(len(df_sorted)), df_sorted['condition'])\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.title('F1 Score by Chronic Condition')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Condition difficulty analysis function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Error Analysis\n",
    "\n",
    "Examine false positives and false negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(results_file, ground_truth_dict, condition, error_type='fp'):\n",
    "    \"\"\"\n",
    "    Analyze false positives or false negatives for a condition.\n",
    "    \n",
    "    Args:\n",
    "        results_file: Path to model results\n",
    "        ground_truth_dict: Ground truth dictionary\n",
    "        condition: Condition to analyze\n",
    "        error_type: 'fp' for false positives, 'fn' for false negatives\n",
    "    \n",
    "    Returns:\n",
    "        List of error cases with details\n",
    "    \"\"\"\n",
    "    # Load results\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    errors = []\n",
    "    ground_truth_set = set(ground_truth_dict.get(condition, []))\n",
    "    \n",
    "    for patient_id, patient_results in results.items():\n",
    "        patient_id_int = int(patient_id)\n",
    "        has_condition = patient_id_int in ground_truth_set\n",
    "        \n",
    "        # Find prediction\n",
    "        predicted = False\n",
    "        rationale = \"\"\n",
    "        confidence = \"\"\n",
    "        \n",
    "        for assessment in patient_results['assessments']:\n",
    "            if assessment['comorbidity'] == condition:\n",
    "                predicted = assessment['is_met']\n",
    "                rationale = assessment.get('rationale', '')\n",
    "                confidence = assessment.get('confidence', '')\n",
    "                break\n",
    "        \n",
    "        # Check for errors\n",
    "        if error_type == 'fp' and predicted and not has_condition:\n",
    "            errors.append({\n",
    "                'patient_id': patient_id,\n",
    "                'error_type': 'False Positive',\n",
    "                'rationale': rationale,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "        elif error_type == 'fn' and not predicted and has_condition:\n",
    "            errors.append({\n",
    "                'patient_id': patient_id,\n",
    "                'error_type': 'False Negative',\n",
    "                'rationale': rationale,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    return errors\n",
    "\n",
    "print(\"Error analysis function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Dependencies and Setup\n",
    "\n",
    "### Required Python Packages\n",
    "\n",
    "```bash\n",
    "pip install requests python-dotenv pandas numpy scikit-learn matplotlib seaborn\n",
    "```\n",
    "\n",
    "### Environment Variables (.env file)\n",
    "\n",
    "Configure based on your LLM API provider:\n",
    "\n",
    "```\n",
    "# Generic LLM API Configuration\n",
    "LLM_API_KEY=your_api_key_here\n",
    "LLM_API_ENDPOINT=https://api.your-provider.com/v1/chat/completions\n",
    "LLM_API_VERSION=2024-01-01\n",
    "```\n",
    "\n",
    "### Data Structure\n",
    "\n",
    "```\n",
    "project/\n",
    "├── data/\n",
    "│   ├── patient_data.json           # Patient clinical notes\n",
    "│   ├── patients_any.pkl            # Ground truth dictionary\n",
    "│   ├── patients_by_condition.pkl   # Diagnosis-based labels\n",
    "│   ├── patients_by_drug_condition.pkl\n",
    "│   ├── patients_by_lab.pkl\n",
    "│   └── patients_by_procedure.pkl\n",
    "├── results/\n",
    "│   └── model_results.json          # Model evaluation results\n",
    "└── .env                            # API credentials\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
