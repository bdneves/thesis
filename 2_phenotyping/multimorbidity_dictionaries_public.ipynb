{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Patient Dictionaries from OMOP CDM Data\n",
    "\n",
    "This notebook demonstrates how to create structured patient dictionaries from OMOP Common Data Model (CDM) data, including:\n",
    "- Condition mappings (ICD-10, SNOMED, CCSR categories)\n",
    "- Procedure mappings\n",
    "- Drug/medication mappings\n",
    "- Lab measurement mappings\n",
    "- **Temporal tracking**: earliest occurrence dates for each clinical event\n",
    "\n",
    "## Use Cases\n",
    "- Multimorbidity research\n",
    "- Chronic disease pattern analysis\n",
    "- Healthcare utilization studies\n",
    "- Cohort identification\n",
    "- Longitudinal health tracking\n",
    "\n",
    "## Requirements\n",
    "- OMOP CDM database (Parquet, CSV, or database connection)\n",
    "- Dictionary definition files (provided separately)\n",
    "- Python libraries: pyarrow, pandas, numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Set, Optional, Any, Tuple\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Update these paths to match your data location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for dictionary creation pipeline.\"\"\"\n",
    "    \n",
    "    # Input: OMOP data directory (adjust to your data location)\n",
    "    omop_data_dir: Path = Path('./omop_data')  # Change this to your OMOP data path\n",
    "    \n",
    "    # Output: where to save generated dictionaries\n",
    "    output_dir: Path = Path('./output/dictionaries')\n",
    "    \n",
    "    # OMOP tables needed\n",
    "    required_tables: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.required_tables is None:\n",
    "            self.required_tables = [\n",
    "                'person',\n",
    "                'condition_occurrence',\n",
    "                'drug_exposure',\n",
    "                'measurement',\n",
    "                'procedure_occurrence',\n",
    "                'visit_occurrence',\n",
    "                'concept',\n",
    "                'concept_ancestor',\n",
    "                'concept_relationship'\n",
    "            ]\n",
    "        \n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Output directory: {self.output_dir}\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Load dictionary definitions (these should be in the same directory)\n",
    "from dictionary_definitions import (\n",
    "    all_conditions_dictionary,\n",
    "    all_procedures_dictionary,\n",
    "    all_drugs_dictionary,\n",
    "    lab_conditions,\n",
    "    rx_risk_dictionary\n",
    ")\n",
    "\n",
    "logger.info(f\"Loaded {len(all_conditions_dictionary)} condition categories\")\n",
    "logger.info(f\"Loaded {len(all_procedures_dictionary)} procedure categories\")\n",
    "logger.info(f\"Loaded {len(all_drugs_dictionary)} drug categories\")\n",
    "logger.info(f\"Loaded {len(lab_conditions)} lab conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null_type_columns(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Remove columns with null/NA type from PyArrow table.\n",
    "    These can cause issues in downstream processing.\n",
    "    \"\"\"\n",
    "    valid_columns = [\n",
    "        name for name, field in zip(table.column_names, table.schema)\n",
    "        if not pa.types.is_null(field.type)\n",
    "    ]\n",
    "    if len(valid_columns) < len(table.column_names):\n",
    "        logger.warning(f\"Removed {len(table.column_names) - len(valid_columns)} null-type columns\")\n",
    "    return table.select(valid_columns)\n",
    "\n",
    "\n",
    "def ensure_int64(table: pa.Table, column_name: str) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Ensure a column is int64 type for consistent comparisons.\n",
    "    \"\"\"\n",
    "    if column_name not in table.column_names:\n",
    "        return table\n",
    "    \n",
    "    col = table[column_name]\n",
    "    if not pa.types.is_integer(col.type):\n",
    "        try:\n",
    "            col = pc.cast(col, pa.int64())\n",
    "            idx = table.column_names.index(column_name)\n",
    "            table = table.set_column(idx, column_name, col)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not cast {column_name} to int64: {e}\")\n",
    "    elif col.type != pa.int64():\n",
    "        col = pc.cast(col, pa.int64())\n",
    "        idx = table.column_names.index(column_name)\n",
    "        table = table.set_column(idx, column_name, col)\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "def load_omop_table(table_name: str, data_dir: Path) -> Optional[pa.Table]:\n",
    "    \"\"\"\n",
    "    Load an OMOP table from Parquet file.\n",
    "    Modify this function if your data is in a different format (CSV, database, etc.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try common file patterns\n",
    "        possible_paths = [\n",
    "            data_dir / f\"{table_name}.parquet\",\n",
    "            data_dir / f\"{table_name.upper()}.parquet\",\n",
    "            data_dir / f\"{table_name}.csv\",\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                if path.suffix == '.parquet':\n",
    "                    table = pq.read_table(path)\n",
    "                elif path.suffix == '.csv':\n",
    "                    df = pd.read_csv(path)\n",
    "                    table = pa.Table.from_pandas(df)\n",
    "                \n",
    "                logger.info(f\"Loaded {table_name}: {table.num_rows:,} rows, {len(table.column_names)} columns\")\n",
    "                return remove_null_type_columns(table)\n",
    "        \n",
    "        logger.warning(f\"Table {table_name} not found in {data_dir}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_dictionary(data: Dict, filename: str, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Save dictionary to JSON file.\n",
    "    \"\"\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    # Convert sets to lists for JSON serialization\n",
    "    if isinstance(data, dict):\n",
    "        data = {k: sorted(list(v)) if isinstance(v, set) else v \n",
    "                for k, v in data.items()}\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Saved {filename} ({len(data)} entries)\")\n",
    "\n",
    "\n",
    "def timestamp_to_date_string(ts_array: pa.Array) -> List[Optional[str]]:\n",
    "    \"\"\"\n",
    "    Convert PyArrow timestamp array to date strings (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "    pyvals = ts_array.to_pylist()\n",
    "    return [\n",
    "        v.strftime(\"%Y-%m-%d\") if isinstance(v, datetime) else None\n",
    "        for v in pyvals\n",
    "    ]\n",
    "\n",
    "logger.info(\"Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OMOPDataLoader:\n",
    "    \"\"\"\n",
    "    Loads and manages OMOP CDM tables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Path):\n",
    "        self.data_dir = data_dir\n",
    "        self.tables = {}\n",
    "        \n",
    "    def load_table(self, table_name: str) -> Optional[pa.Table]:\n",
    "        \"\"\"Load a single OMOP table.\"\"\"\n",
    "        if table_name not in self.tables:\n",
    "            self.tables[table_name] = load_omop_table(table_name, self.data_dir)\n",
    "        return self.tables[table_name]\n",
    "    \n",
    "    def load_all_required_tables(self, table_list: List[str]):\n",
    "        \"\"\"Load all required tables.\"\"\"\n",
    "        logger.info(f\"Loading {len(table_list)} OMOP tables...\")\n",
    "        for table_name in table_list:\n",
    "            self.load_table(table_name)\n",
    "        logger.info(\"All tables loaded\")\n",
    "    \n",
    "    def get_valid_person_ids(self) -> Set[int]:\n",
    "        \"\"\"Get set of valid person IDs from person table.\"\"\"\n",
    "        person_table = self.load_table('person')\n",
    "        if person_table is None:\n",
    "            logger.error(\"Person table not found!\")\n",
    "            return set()\n",
    "        \n",
    "        # Adjust column name based on your data (might be PERSON_ID or person_id)\n",
    "        person_id_col = None\n",
    "        for col in ['PERSON_ID', 'person_id', 'PersonId']:\n",
    "            if col in person_table.column_names:\n",
    "                person_id_col = col\n",
    "                break\n",
    "        \n",
    "        if person_id_col is None:\n",
    "            logger.error(\"Could not find person_id column\")\n",
    "            return set()\n",
    "        \n",
    "        person_ids = set(person_table[person_id_col].to_pylist())\n",
    "        logger.info(f\"Found {len(person_ids):,} valid persons\")\n",
    "        return person_ids\n",
    "\n",
    "logger.info(\"Data loader class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load OMOP Data\n",
    "\n",
    "**Note**: Modify the `config.omop_data_dir` path above to point to your OMOP data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = OMOPDataLoader(config.omop_data_dir)\n",
    "\n",
    "# Load all required tables\n",
    "loader.load_all_required_tables(config.required_tables)\n",
    "\n",
    "# Get valid person IDs\n",
    "valid_person_ids = loader.get_valid_person_ids()\n",
    "\n",
    "logger.info(f\"Data loading complete. Working with {len(valid_person_ids):,} patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Condition Dictionaries\n",
    "\n",
    "Maps conditions (diagnoses) to patients, tracking earliest occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_condition_dictionaries(loader: OMOPDataLoader, \n",
    "                                 condition_dict: Dict,\n",
    "                                 output_dir: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Create patient dictionaries for conditions with earliest dates.\n",
    "    \n",
    "    Args:\n",
    "        loader: OMOP data loader\n",
    "        condition_dict: Dictionary mapping category names to concept IDs\n",
    "        output_dir: Where to save output files\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with patient mappings and dates\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating condition dictionaries...\")\n",
    "    \n",
    "    # Load condition_occurrence table\n",
    "    cond_table = loader.load_table('condition_occurrence')\n",
    "    if cond_table is None:\n",
    "        logger.error(\"condition_occurrence table not found\")\n",
    "        return {}\n",
    "    \n",
    "    # Standardize column names (adjust based on your data)\n",
    "    col_mapping = {\n",
    "        'person_id': None,\n",
    "        'condition_concept_id': None,\n",
    "        'condition_start_date': None\n",
    "    }\n",
    "    \n",
    "    for standard_name in col_mapping.keys():\n",
    "        for col in cond_table.column_names:\n",
    "            if col.lower() == standard_name or col.lower() == standard_name.upper():\n",
    "                col_mapping[standard_name] = col\n",
    "                break\n",
    "    \n",
    "    if any(v is None for v in col_mapping.values()):\n",
    "        logger.error(f\"Missing required columns. Found: {cond_table.column_names}\")\n",
    "        return {}\n",
    "    \n",
    "    # Select and rename columns\n",
    "    cond_table = cond_table.select(list(col_mapping.values()))\n",
    "    cond_table = cond_table.rename_columns(list(col_mapping.keys()))\n",
    "    \n",
    "    # Ensure proper types\n",
    "    cond_table = ensure_int64(cond_table, 'person_id')\n",
    "    cond_table = ensure_int64(cond_table, 'condition_concept_id')\n",
    "    \n",
    "    # Parse dates\n",
    "    date_col = pc.cast(cond_table['condition_start_date'], pa.string())\n",
    "    date_parsed = pc.strptime(date_col, format='%Y-%m-%d', unit='s')\n",
    "    cond_table = cond_table.append_column('date_parsed', date_parsed)\n",
    "    \n",
    "    # Filter to valid patients\n",
    "    valid_mask = pc.is_in(\n",
    "        cond_table['person_id'],\n",
    "        pa.array(list(valid_person_ids), type=pa.int64())\n",
    "    )\n",
    "    cond_table = cond_table.filter(valid_mask)\n",
    "    \n",
    "    logger.info(f\"Processing {cond_table.num_rows:,} condition records\")\n",
    "    \n",
    "    # Create dictionaries for each category\n",
    "    results = {\n",
    "        'patients_by_condition': {},\n",
    "        'patients_by_condition_first_date': {}\n",
    "    }\n",
    "    \n",
    "    for category_name, concept_ids in condition_dict.items():\n",
    "        # Filter to this category's concepts\n",
    "        concept_mask = pc.is_in(\n",
    "            cond_table['condition_concept_id'],\n",
    "            pa.array(concept_ids, type=pa.int64())\n",
    "        )\n",
    "        category_table = cond_table.filter(concept_mask)\n",
    "        \n",
    "        if category_table.num_rows == 0:\n",
    "            continue\n",
    "        \n",
    "        # Group by person and get earliest date\n",
    "        grouped = category_table.group_by('person_id').aggregate([\n",
    "            ('date_parsed', 'min')\n",
    "        ])\n",
    "        \n",
    "        # Extract results\n",
    "        person_ids = grouped['person_id'].to_pylist()\n",
    "        dates = timestamp_to_date_string(grouped['date_parsed_min'])\n",
    "        \n",
    "        results['patients_by_condition'][category_name] = sorted(person_ids)\n",
    "        results['patients_by_condition_first_date'][category_name] = dict(zip(person_ids, dates))\n",
    "    \n",
    "    # Save results\n",
    "    save_dictionary(results['patients_by_condition'], 'conditions.json', output_dir)\n",
    "    save_dictionary(results['patients_by_condition_first_date'], 'conditions_first_date.json', output_dir)\n",
    "    \n",
    "    logger.info(f\"Created dictionaries for {len(results['patients_by_condition'])} condition categories\")\n",
    "    return results\n",
    "\n",
    "# Run condition dictionary creation\n",
    "condition_results = create_condition_dictionaries(\n",
    "    loader, \n",
    "    all_conditions_dictionary, \n",
    "    config.output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Procedure Dictionaries\n",
    "\n",
    "Maps procedures to patients with earliest occurrence dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_procedure_dictionaries(loader: OMOPDataLoader,\n",
    "                                 procedure_dict: Dict,\n",
    "                                 output_dir: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Create patient dictionaries for procedures with earliest dates.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating procedure dictionaries...\")\n",
    "    \n",
    "    proc_table = loader.load_table('procedure_occurrence')\n",
    "    if proc_table is None:\n",
    "        logger.error(\"procedure_occurrence table not found\")\n",
    "        return {}\n",
    "    \n",
    "    # Similar processing as conditions\n",
    "    col_mapping = {\n",
    "        'person_id': None,\n",
    "        'procedure_concept_id': None,\n",
    "        'procedure_date': None\n",
    "    }\n",
    "    \n",
    "    for standard_name in col_mapping.keys():\n",
    "        for col in proc_table.column_names:\n",
    "            if col.lower() == standard_name or col.lower() == standard_name.upper():\n",
    "                col_mapping[standard_name] = col\n",
    "                break\n",
    "    \n",
    "    if any(v is None for v in col_mapping.values()):\n",
    "        logger.error(f\"Missing required columns\")\n",
    "        return {}\n",
    "    \n",
    "    proc_table = proc_table.select(list(col_mapping.values()))\n",
    "    proc_table = proc_table.rename_columns(list(col_mapping.keys()))\n",
    "    \n",
    "    proc_table = ensure_int64(proc_table, 'person_id')\n",
    "    proc_table = ensure_int64(proc_table, 'procedure_concept_id')\n",
    "    \n",
    "    # Parse dates\n",
    "    date_col = pc.cast(proc_table['procedure_date'], pa.string())\n",
    "    date_parsed = pc.strptime(date_col, format='%Y-%m-%d', unit='s')\n",
    "    proc_table = proc_table.append_column('date_parsed', date_parsed)\n",
    "    \n",
    "    # Filter valid patients\n",
    "    valid_mask = pc.is_in(\n",
    "        proc_table['person_id'],\n",
    "        pa.array(list(valid_person_ids), type=pa.int64())\n",
    "    )\n",
    "    proc_table = proc_table.filter(valid_mask)\n",
    "    \n",
    "    logger.info(f\"Processing {proc_table.num_rows:,} procedure records\")\n",
    "    \n",
    "    results = {\n",
    "        'patients_by_procedure': {},\n",
    "        'patients_by_procedure_first_date': {}\n",
    "    }\n",
    "    \n",
    "    for category_name, concept_ids in procedure_dict.items():\n",
    "        concept_mask = pc.is_in(\n",
    "            proc_table['procedure_concept_id'],\n",
    "            pa.array(concept_ids, type=pa.int64())\n",
    "        )\n",
    "        category_table = proc_table.filter(concept_mask)\n",
    "        \n",
    "        if category_table.num_rows == 0:\n",
    "            continue\n",
    "        \n",
    "        grouped = category_table.group_by('person_id').aggregate([\n",
    "            ('date_parsed', 'min')\n",
    "        ])\n",
    "        \n",
    "        person_ids = grouped['person_id'].to_pylist()\n",
    "        dates = timestamp_to_date_string(grouped['date_parsed_min'])\n",
    "        \n",
    "        results['patients_by_procedure'][category_name] = sorted(person_ids)\n",
    "        results['patients_by_procedure_first_date'][category_name] = dict(zip(person_ids, dates))\n",
    "    \n",
    "    save_dictionary(results['patients_by_procedure'], 'procedures.json', output_dir)\n",
    "    save_dictionary(results['patients_by_procedure_first_date'], 'procedures_first_date.json', output_dir)\n",
    "    \n",
    "    logger.info(f\"Created dictionaries for {len(results['patients_by_procedure'])} procedure categories\")\n",
    "    return results\n",
    "\n",
    "# Run procedure dictionary creation\n",
    "procedure_results = create_procedure_dictionaries(\n",
    "    loader,\n",
    "    all_procedures_dictionary,\n",
    "    config.output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Drug/Medication Dictionaries\n",
    "\n",
    "Maps medications to patients with earliest prescription/exposure dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drug_dictionaries(loader: OMOPDataLoader,\n",
    "                            drug_dict: Dict,\n",
    "                            output_dir: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Create patient dictionaries for drugs/medications with earliest dates.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating drug dictionaries...\")\n",
    "    \n",
    "    drug_table = loader.load_table('drug_exposure')\n",
    "    if drug_table is None:\n",
    "        logger.error(\"drug_exposure table not found\")\n",
    "        return {}\n",
    "    \n",
    "    col_mapping = {\n",
    "        'person_id': None,\n",
    "        'drug_concept_id': None,\n",
    "        'drug_exposure_start_date': None\n",
    "    }\n",
    "    \n",
    "    for standard_name in col_mapping.keys():\n",
    "        for col in drug_table.column_names:\n",
    "            if col.lower() == standard_name or col.lower() == standard_name.upper():\n",
    "                col_mapping[standard_name] = col\n",
    "                break\n",
    "    \n",
    "    if any(v is None for v in col_mapping.values()):\n",
    "        logger.error(f\"Missing required columns\")\n",
    "        return {}\n",
    "    \n",
    "    drug_table = drug_table.select(list(col_mapping.values()))\n",
    "    drug_table = drug_table.rename_columns(list(col_mapping.keys()))\n",
    "    \n",
    "    drug_table = ensure_int64(drug_table, 'person_id')\n",
    "    drug_table = ensure_int64(drug_table, 'drug_concept_id')\n",
    "    \n",
    "    # Parse dates\n",
    "    date_col = pc.cast(drug_table['drug_exposure_start_date'], pa.string())\n",
    "    date_parsed = pc.strptime(date_col, format='%Y-%m-%d', unit='s')\n",
    "    drug_table = drug_table.append_column('date_parsed', date_parsed)\n",
    "    \n",
    "    # Filter valid patients\n",
    "    valid_mask = pc.is_in(\n",
    "        drug_table['person_id'],\n",
    "        pa.array(list(valid_person_ids), type=pa.int64())\n",
    "    )\n",
    "    drug_table = drug_table.filter(valid_mask)\n",
    "    \n",
    "    logger.info(f\"Processing {drug_table.num_rows:,} drug exposure records\")\n",
    "    \n",
    "    results = {\n",
    "        'patients_by_drug': {},\n",
    "        'patients_by_drug_first_date': {}\n",
    "    }\n",
    "    \n",
    "    for category_name, concept_ids in drug_dict.items():\n",
    "        concept_mask = pc.is_in(\n",
    "            drug_table['drug_concept_id'],\n",
    "            pa.array(concept_ids, type=pa.int64())\n",
    "        )\n",
    "        category_table = drug_table.filter(concept_mask)\n",
    "        \n",
    "        if category_table.num_rows == 0:\n",
    "            continue\n",
    "        \n",
    "        grouped = category_table.group_by('person_id').aggregate([\n",
    "            ('date_parsed', 'min')\n",
    "        ])\n",
    "        \n",
    "        person_ids = grouped['person_id'].to_pylist()\n",
    "        dates = timestamp_to_date_string(grouped['date_parsed_min'])\n",
    "        \n",
    "        results['patients_by_drug'][category_name] = sorted(person_ids)\n",
    "        results['patients_by_drug_first_date'][category_name] = dict(zip(person_ids, dates))\n",
    "    \n",
    "    save_dictionary(results['patients_by_drug'], 'drugs.json', output_dir)\n",
    "    save_dictionary(results['patients_by_drug_first_date'], 'drugs_first_date.json', output_dir)\n",
    "    \n",
    "    logger.info(f\"Created dictionaries for {len(results['patients_by_drug'])} drug categories\")\n",
    "    return results\n",
    "\n",
    "# Run drug dictionary creation\n",
    "drug_results = create_drug_dictionaries(\n",
    "    loader,\n",
    "    all_drugs_dictionary,\n",
    "    config.output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Lab Measurement Dictionaries\n",
    "\n",
    "Maps lab measurements (e.g., abnormal values) to patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lab_dictionaries(loader: OMOPDataLoader,\n",
    "                           lab_list: List,\n",
    "                           output_dir: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Create patient dictionaries for lab measurements.\n",
    "    Note: This is a simplified version. Customize based on your lab definitions.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating lab measurement dictionaries...\")\n",
    "    \n",
    "    meas_table = loader.load_table('measurement')\n",
    "    if meas_table is None:\n",
    "        logger.error(\"measurement table not found\")\n",
    "        return {}\n",
    "    \n",
    "    col_mapping = {\n",
    "        'person_id': None,\n",
    "        'measurement_concept_id': None,\n",
    "        'measurement_date': None,\n",
    "        'value_as_number': None\n",
    "    }\n",
    "    \n",
    "    for standard_name in col_mapping.keys():\n",
    "        for col in meas_table.column_names:\n",
    "            if col.lower() == standard_name or col.lower() == standard_name.upper():\n",
    "                col_mapping[standard_name] = col\n",
    "                break\n",
    "    \n",
    "    if any(v is None for v in col_mapping.values()):\n",
    "        logger.error(f\"Missing required columns\")\n",
    "        return {}\n",
    "    \n",
    "    meas_table = meas_table.select(list(col_mapping.values()))\n",
    "    meas_table = meas_table.rename_columns(list(col_mapping.keys()))\n",
    "    \n",
    "    meas_table = ensure_int64(meas_table, 'person_id')\n",
    "    meas_table = ensure_int64(meas_table, 'measurement_concept_id')\n",
    "    \n",
    "    # Parse dates\n",
    "    date_col = pc.cast(meas_table['measurement_date'], pa.string())\n",
    "    date_parsed = pc.strptime(date_col, format='%Y-%m-%d', unit='s')\n",
    "    meas_table = meas_table.append_column('date_parsed', date_parsed)\n",
    "    \n",
    "    # Filter valid patients\n",
    "    valid_mask = pc.is_in(\n",
    "        meas_table['person_id'],\n",
    "        pa.array(list(valid_person_ids), type=pa.int64())\n",
    "    )\n",
    "    meas_table = meas_table.filter(valid_mask)\n",
    "    \n",
    "    logger.info(f\"Processing {meas_table.num_rows:,} measurement records\")\n",
    "    \n",
    "    results = {\n",
    "        'patients_by_lab': {},\n",
    "        'patients_by_lab_first_date': {}\n",
    "    }\n",
    "    \n",
    "    # Process lab_list (structure depends on your lab_conditions definition)\n",
    "    # This is a placeholder - customize based on your needs\n",
    "    for lab_item in lab_list:\n",
    "        if isinstance(lab_item, dict) and 'concept_id' in lab_item:\n",
    "            concept_id = lab_item['concept_id']\n",
    "            name = lab_item.get('name', f'lab_{concept_id}')\n",
    "            \n",
    "            concept_mask = pc.equal(\n",
    "                meas_table['measurement_concept_id'],\n",
    "                pa.scalar(concept_id, type=pa.int64())\n",
    "            )\n",
    "            category_table = meas_table.filter(concept_mask)\n",
    "            \n",
    "            if category_table.num_rows == 0:\n",
    "                continue\n",
    "            \n",
    "            grouped = category_table.group_by('person_id').aggregate([\n",
    "                ('date_parsed', 'min')\n",
    "            ])\n",
    "            \n",
    "            person_ids = grouped['person_id'].to_pylist()\n",
    "            dates = timestamp_to_date_string(grouped['date_parsed_min'])\n",
    "            \n",
    "            results['patients_by_lab'][name] = sorted(person_ids)\n",
    "            results['patients_by_lab_first_date'][name] = dict(zip(person_ids, dates))\n",
    "    \n",
    "    save_dictionary(results['patients_by_lab'], 'labs.json', output_dir)\n",
    "    save_dictionary(results['patients_by_lab_first_date'], 'labs_first_date.json', output_dir)\n",
    "    \n",
    "    logger.info(f\"Created dictionaries for {len(results['patients_by_lab'])} lab categories\")\n",
    "    return results\n",
    "\n",
    "# Run lab dictionary creation\n",
    "lab_results = create_lab_dictionaries(\n",
    "    loader,\n",
    "    lab_conditions,\n",
    "    config.output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Output Files\n",
    "\n",
    "Review the generated dictionary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "output_files = list(config.output_dir.glob('*.json'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DICTIONARY CREATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutput directory: {config.output_dir}\")\n",
    "print(f\"\\nGenerated {len(output_files)} dictionary files:\\n\")\n",
    "\n",
    "for file in sorted(output_files):\n",
    "    size_kb = file.stat().st_size / 1024\n",
    "    print(f\"  • {file.name:<40} ({size_kb:>8.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nDictionary Structure:\")\n",
    "print(\"  - conditions.json: {category: [patient_ids]}\")\n",
    "print(\"  - conditions_first_date.json: {category: {patient_id: date}}\")\n",
    "print(\"  - procedures.json: {category: [patient_ids]}\")\n",
    "print(\"  - procedures_first_date.json: {category: {patient_id: date}}\")\n",
    "print(\"  - drugs.json: {category: [patient_ids]}\")\n",
    "print(\"  - drugs_first_date.json: {category: {patient_id: date}}\")\n",
    "print(\"  - labs.json: {category: [patient_ids]}\")\n",
    "print(\"  - labs_first_date.json: {category: {patient_id: date}}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample output\n",
    "if output_files:\n",
    "    sample_file = [f for f in output_files if 'first_date' not in f.name][0]\n",
    "    with open(sample_file, 'r') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nSample from {sample_file.name}:\")\n",
    "    print(f\"Categories: {list(sample_data.keys())[:5]}\")\n",
    "    \n",
    "    if sample_data:\n",
    "        first_category = list(sample_data.keys())[0]\n",
    "        print(f\"\\nExample - '{first_category}':\")\n",
    "        print(f\"  Patient count: {len(sample_data[first_category])}\")\n",
    "        print(f\"  Sample IDs: {sample_data[first_category][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Usage\n",
    "1. Place your OMOP data files in the specified directory\n",
    "2. Ensure `dictionary_definitions.py` is in the same directory\n",
    "3. Run all cells sequentially\n",
    "4. Dictionary JSON files will be created in `./output/dictionaries/`\n",
    "\n",
    "### Dictionary Structure\n",
    "Each dictionary maps medical categories to patients:\n",
    "- **Main dictionaries**: `{category: [patient_id_list]}`\n",
    "- **Date dictionaries**: `{category: {patient_id: first_occurrence_date}}`\n",
    "\n",
    "### Customization\n",
    "- Modify `Config` class to change input/output paths\n",
    "- Update `load_omop_table()` for different data formats (CSV, database, etc.)\n",
    "- Adjust column name mappings if your OMOP implementation uses different names\n",
    "- Add additional filtering logic as needed (e.g., date ranges, specific cohorts)\n",
    "\n",
    "### Performance\n",
    "- Uses PyArrow for efficient processing of large datasets\n",
    "- Processes millions of records in minutes\n",
    "- Memory-efficient with streaming operations\n",
    "\n",
    "### Output Files\n",
    "Generated dictionaries can be used for:\n",
    "- Cohort identification\n",
    "- Feature engineering for ML models\n",
    "- Multimorbidity analysis\n",
    "- Healthcare utilization studies\n",
    "- Longitudinal outcome tracking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
