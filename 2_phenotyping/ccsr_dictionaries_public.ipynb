{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating CCSR-Based Patient Dictionaries from OMOP Data\n",
    "\n",
    "This notebook creates patient dictionaries using **CCSR (Clinical Classifications Software Refined)** categories from OMOP Common Data Model data.\n",
    "\n",
    "## What is CCSR?\n",
    "CCSR is a diagnosis and procedure categorization scheme developed by AHRQ (Agency for Healthcare Research and Quality) that:\n",
    "- Groups ICD-10 codes into clinically meaningful categories\n",
    "- Provides hierarchical organization of diagnoses\n",
    "- Distinguishes between chronic and acute conditions\n",
    "- Separates inpatient (IP) and outpatient (OP) encounters\n",
    "\n",
    "## Output\n",
    "This notebook generates:\n",
    "- **Chronic condition dictionaries** by CCSR category\n",
    "- **Inpatient vs Outpatient** separate mappings\n",
    "- **First occurrence dates** for each patient-condition pair\n",
    "\n",
    "## Requirements\n",
    "- OMOP CDM database\n",
    "- SNOMED to CCSR mapping file\n",
    "- Chronic conditions indicator file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Set, Optional, Any, Tuple\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for CCSR dictionary creation.\"\"\"\n",
    "    \n",
    "    # OMOP data directory\n",
    "    omop_data_dir: Path = Path('./omop_data')\n",
    "    \n",
    "    # CCSR mapping files directory\n",
    "    ccsr_mappings_dir: Path = Path('./ccsr_mappings')\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir: Path = Path('./output/ccsr_dictionaries')\n",
    "    \n",
    "    # Required OMOP tables\n",
    "    required_tables: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.required_tables is None:\n",
    "            self.required_tables = [\n",
    "                'person',\n",
    "                'condition_occurrence',\n",
    "                'visit_occurrence',\n",
    "                'concept',\n",
    "                'concept_relationship'\n",
    "            ]\n",
    "        \n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Output directory: {self.output_dir}\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Note: You need to provide CCSR mapping files:\n",
    "# 1. snomed_to_ccsr_mapping.csv - Maps SNOMED codes to CCSR categories\n",
    "# 2. ccsr_chronic_conditions.csv - List of chronic CCSR categories\n",
    "logger.info(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null_type_columns(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"Remove columns with null/NA type from PyArrow table.\"\"\"\n",
    "    valid_columns = [\n",
    "        name for name, field in zip(table.column_names, table.schema)\n",
    "        if not pa.types.is_null(field.type)\n",
    "    ]\n",
    "    if len(valid_columns) < len(table.column_names):\n",
    "        logger.warning(f\"Removed {len(table.column_names) - len(valid_columns)} null-type columns\")\n",
    "    return table.select(valid_columns)\n",
    "\n",
    "\n",
    "def ensure_int64(table: pa.Table, column_name: str) -> pa.Table:\n",
    "    \"\"\"Ensure a column is int64 type.\"\"\"\n",
    "    if column_name not in table.column_names:\n",
    "        return table\n",
    "    \n",
    "    col = table[column_name]\n",
    "    if not pa.types.is_integer(col.type) or col.type != pa.int64():\n",
    "        try:\n",
    "            col = pc.cast(col, pa.int64())\n",
    "            idx = table.column_names.index(column_name)\n",
    "            table = table.set_column(idx, column_name, col)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not cast {column_name} to int64: {e}\")\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "def load_omop_table(table_name: str, data_dir: Path) -> Optional[pa.Table]:\n",
    "    \"\"\"Load an OMOP table from Parquet or CSV file.\"\"\"\n",
    "    try:\n",
    "        possible_paths = [\n",
    "            data_dir / f\"{table_name}.parquet\",\n",
    "            data_dir / f\"{table_name.upper()}.parquet\",\n",
    "            data_dir / f\"{table_name}.csv\",\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                if path.suffix == '.parquet':\n",
    "                    table = pq.read_table(path)\n",
    "                elif path.suffix == '.csv':\n",
    "                    df = pd.read_csv(path)\n",
    "                    table = pa.Table.from_pandas(df)\n",
    "                \n",
    "                logger.info(f\"Loaded {table_name}: {table.num_rows:,} rows\")\n",
    "                return remove_null_type_columns(table)\n",
    "        \n",
    "        logger.warning(f\"Table {table_name} not found\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def timestamp_to_date_string(ts_array: pa.Array) -> List[Optional[str]]:\n",
    "    \"\"\"Convert PyArrow timestamp array to date strings.\"\"\"\n",
    "    pyvals = ts_array.to_pylist()\n",
    "    return [\n",
    "        v.strftime(\"%Y-%m-%d\") if isinstance(v, datetime) else None\n",
    "        for v in pyvals\n",
    "    ]\n",
    "\n",
    "\n",
    "def save_dictionary(data: Dict, filename: str, output_dir: Path):\n",
    "    \"\"\"Save dictionary to JSON file.\"\"\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    # Convert sets to lists\n",
    "    if isinstance(data, dict):\n",
    "        data = {k: sorted(list(v)) if isinstance(v, set) else v \n",
    "                for k, v in data.items()}\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Saved {filename} ({len(data)} categories)\")\n",
    "\n",
    "logger.info(\"Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OMOPDataLoader:\n",
    "    \"\"\"Loads and manages OMOP CDM tables.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Path):\n",
    "        self.data_dir = data_dir\n",
    "        self.tables = {}\n",
    "        \n",
    "    def load_table(self, table_name: str) -> Optional[pa.Table]:\n",
    "        \"\"\"Load a single OMOP table.\"\"\"\n",
    "        if table_name not in self.tables:\n",
    "            self.tables[table_name] = load_omop_table(table_name, self.data_dir)\n",
    "        return self.tables[table_name]\n",
    "    \n",
    "    def load_all_required_tables(self, table_list: List[str]):\n",
    "        \"\"\"Load all required tables.\"\"\"\n",
    "        logger.info(f\"Loading {len(table_list)} OMOP tables...\")\n",
    "        for table_name in table_list:\n",
    "            self.load_table(table_name)\n",
    "        logger.info(\"All tables loaded\")\n",
    "    \n",
    "    def get_valid_person_ids(self) -> Set[int]:\n",
    "        \"\"\"Get set of valid person IDs.\"\"\"\n",
    "        person_table = self.load_table('person')\n",
    "        if person_table is None:\n",
    "            logger.error(\"Person table not found!\")\n",
    "            return set()\n",
    "        \n",
    "        # Find person_id column\n",
    "        person_id_col = None\n",
    "        for col in ['PERSON_ID', 'person_id', 'PersonId']:\n",
    "            if col in person_table.column_names:\n",
    "                person_id_col = col\n",
    "                break\n",
    "        \n",
    "        if person_id_col is None:\n",
    "            logger.error(\"Could not find person_id column\")\n",
    "            return set()\n",
    "        \n",
    "        person_ids = set(person_table[person_id_col].to_pylist())\n",
    "        logger.info(f\"Found {len(person_ids):,} valid persons\")\n",
    "        return person_ids\n",
    "\n",
    "logger.info(\"Data loader class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load CCSR Mapping Files\n",
    "\n",
    "You need to provide two mapping files in the `ccsr_mappings` directory:\n",
    "\n",
    "### 1. `snomed_to_ccsr_mapping.csv`\n",
    "Maps SNOMED concept IDs to CCSR categories. Expected columns:\n",
    "- `snomed_concept_id` (int): OMOP/SNOMED concept ID\n",
    "- `ccsr_category_ip` (str): Inpatient CCSR category code\n",
    "- `ccsr_category_op` (str): Outpatient CCSR category code\n",
    "\n",
    "### 2. `ccsr_chronic_conditions.csv`\n",
    "List of CCSR categories considered chronic. Expected columns:\n",
    "- `ccsr_category` (str): CCSR category code\n",
    "- `is_chronic` (bool): Whether condition is chronic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ccsr_mappings(mappings_dir: Path) -> Tuple[pa.Table, Set[str]]:\n",
    "    \"\"\"\n",
    "    Load CCSR mapping files.\n",
    "    \n",
    "    Returns:\n",
    "        snomed_ccsr_map: PyArrow table with SNOMED to CCSR mappings\n",
    "        chronic_categories: Set of chronic CCSR category codes\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading CCSR mapping files...\")\n",
    "    \n",
    "    # Load SNOMED to CCSR mapping\n",
    "    snomed_ccsr_path = mappings_dir / 'snomed_to_ccsr_mapping.csv'\n",
    "    if not snomed_ccsr_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"SNOMED to CCSR mapping not found: {snomed_ccsr_path}\\n\"\n",
    "            \"Please provide a CSV with columns: snomed_concept_id, ccsr_category_ip, ccsr_category_op\"\n",
    "        )\n",
    "    \n",
    "    # Read mapping file\n",
    "    snomed_ccsr_df = pd.read_csv(snomed_ccsr_path)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['snomed_concept_id', 'ccsr_category_ip', 'ccsr_category_op']\n",
    "    missing_cols = set(required_cols) - set(snomed_ccsr_df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in SNOMED-CCSR mapping: {missing_cols}\")\n",
    "    \n",
    "    # Convert to PyArrow\n",
    "    snomed_ccsr_map = pa.Table.from_pandas(snomed_ccsr_df[required_cols])\n",
    "    snomed_ccsr_map = snomed_ccsr_map.rename_columns([\n",
    "        'snomed_concept_id',\n",
    "        'ip_cat',\n",
    "        'op_cat'\n",
    "    ])\n",
    "    \n",
    "    # Ensure snomed_concept_id is int64\n",
    "    snomed_ccsr_map = ensure_int64(snomed_ccsr_map, 'snomed_concept_id')\n",
    "    \n",
    "    logger.info(f\"Loaded {snomed_ccsr_map.num_rows:,} SNOMED-CCSR mappings\")\n",
    "    \n",
    "    # Load chronic conditions list\n",
    "    chronic_path = mappings_dir / 'ccsr_chronic_conditions.csv'\n",
    "    if not chronic_path.exists():\n",
    "        logger.warning(\n",
    "            f\"Chronic conditions file not found: {chronic_path}\\n\"\n",
    "            \"All CCSR categories will be considered (no chronic filtering)\"\n",
    "        )\n",
    "        # Extract all unique categories from mapping\n",
    "        chronic_categories = set(\n",
    "            snomed_ccsr_map['ip_cat'].to_pylist() + \n",
    "            snomed_ccsr_map['op_cat'].to_pylist()\n",
    "        )\n",
    "        chronic_categories.discard(None)\n",
    "    else:\n",
    "        chronic_df = pd.read_csv(chronic_path)\n",
    "        chronic_categories = set(\n",
    "            chronic_df[chronic_df['is_chronic'] == True]['ccsr_category'].tolist()\n",
    "        )\n",
    "    \n",
    "    logger.info(f\"Identified {len(chronic_categories)} chronic CCSR categories\")\n",
    "    \n",
    "    return snomed_ccsr_map, chronic_categories\n",
    "\n",
    "# Load mappings\n",
    "snomed_ccsr_map, chronic_categories = load_ccsr_mappings(config.ccsr_mappings_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load OMOP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = OMOPDataLoader(config.omop_data_dir)\n",
    "\n",
    "# Load all required tables\n",
    "loader.load_all_required_tables(config.required_tables)\n",
    "\n",
    "# Get valid person IDs\n",
    "valid_person_ids = loader.get_valid_person_ids()\n",
    "\n",
    "logger.info(f\"Data loading complete. Working with {len(valid_person_ids):,} patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filter CCSR Mapping to Chronic Conditions Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_chronic_categories(snomed_ccsr_map: pa.Table, \n",
    "                                 chronic_set: Set[str]) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filter SNOMED-CCSR mapping to include only chronic conditions.\n",
    "    \"\"\"\n",
    "    logger.info(\"Filtering to chronic conditions only...\")\n",
    "    \n",
    "    # Create boolean masks for chronic categories\n",
    "    ip_categories = snomed_ccsr_map['ip_cat'].to_pylist()\n",
    "    op_categories = snomed_ccsr_map['op_cat'].to_pylist()\n",
    "    \n",
    "    # Keep rows where either IP or OP category is chronic\n",
    "    chronic_mask = [\n",
    "        (ip_cat in chronic_set if ip_cat else False) or \n",
    "        (op_cat in chronic_set if op_cat else False)\n",
    "        for ip_cat, op_cat in zip(ip_categories, op_categories)\n",
    "    ]\n",
    "    \n",
    "    filtered_map = snomed_ccsr_map.filter(pa.array(chronic_mask))\n",
    "    \n",
    "    logger.info(\n",
    "        f\"Filtered from {snomed_ccsr_map.num_rows:,} to \"\n",
    "        f\"{filtered_map.num_rows:,} chronic condition mappings\"\n",
    "    )\n",
    "    \n",
    "    return filtered_map\n",
    "\n",
    "# Filter to chronic conditions\n",
    "snomed_ccsr_chronic = filter_to_chronic_categories(snomed_ccsr_map, chronic_categories)\n",
    "\n",
    "# Remove rows with null SNOMED IDs\n",
    "snomed_ccsr_chronic = snomed_ccsr_chronic.filter(\n",
    "    pc.is_valid(snomed_ccsr_chronic['snomed_concept_id'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Join Condition Occurrences with CCSR Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_conditions_with_ccsr(loader: OMOPDataLoader,\n",
    "                             snomed_ccsr_map: pa.Table,\n",
    "                             valid_person_ids: Set[int]) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Join condition_occurrence with CCSR mappings and visit information.\n",
    "    \"\"\"\n",
    "    logger.info(\"Joining conditions with CCSR categories...\")\n",
    "    \n",
    "    # Load condition_occurrence\n",
    "    condition_table = loader.load_table('condition_occurrence')\n",
    "    if condition_table is None:\n",
    "        raise ValueError(\"condition_occurrence table not found\")\n",
    "    \n",
    "    condition_table = remove_null_type_columns(condition_table)\n",
    "    \n",
    "    # Find required columns (case-insensitive)\n",
    "    col_map = {}\n",
    "    for std_col in ['PERSON_ID', 'VISIT_OCCURRENCE_ID', 'CONDITION_CONCEPT_ID', 'CONDITION_START_DATE']:\n",
    "        for actual_col in condition_table.column_names:\n",
    "            if actual_col.upper() == std_col:\n",
    "                col_map[std_col] = actual_col\n",
    "                break\n",
    "    \n",
    "    missing = set(['PERSON_ID', 'VISIT_OCCURRENCE_ID', 'CONDITION_CONCEPT_ID', 'CONDITION_START_DATE']) - set(col_map.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in condition_occurrence: {missing}\")\n",
    "    \n",
    "    # Select and rename columns\n",
    "    cond_proj = condition_table.select(list(col_map.values())).rename_columns([\n",
    "        'person_id',\n",
    "        'visit_occurrence_id',\n",
    "        'snomed_concept_id',\n",
    "        'condition_start_date'\n",
    "    ])\n",
    "    \n",
    "    # Ensure proper types\n",
    "    cond_proj = ensure_int64(cond_proj, 'snomed_concept_id')\n",
    "    \n",
    "    # Join with CCSR mapping\n",
    "    cond_with_ccsr = cond_proj.join(\n",
    "        snomed_ccsr_map,\n",
    "        keys='snomed_concept_id',\n",
    "        join_type='inner'\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Matched {cond_with_ccsr.num_rows:,} conditions to CCSR categories\")\n",
    "    \n",
    "    # Load visit_occurrence for IP/OP distinction\n",
    "    visit_table = loader.load_table('visit_occurrence')\n",
    "    if visit_table is None:\n",
    "        raise ValueError(\"visit_occurrence table not found\")\n",
    "    \n",
    "    visit_table = remove_null_type_columns(visit_table)\n",
    "    \n",
    "    # Find visit columns\n",
    "    visit_col_map = {}\n",
    "    for std_col in ['VISIT_OCCURRENCE_ID', 'VISIT_CONCEPT_ID']:\n",
    "        for actual_col in visit_table.column_names:\n",
    "            if actual_col.upper() == std_col:\n",
    "                visit_col_map[std_col] = actual_col\n",
    "                break\n",
    "    \n",
    "    if len(visit_col_map) < 2:\n",
    "        raise ValueError(\"Missing VISIT_OCCURRENCE_ID or VISIT_CONCEPT_ID in visit_occurrence\")\n",
    "    \n",
    "    visit_proj = visit_table.select(list(visit_col_map.values())).rename_columns([\n",
    "        'visit_occurrence_id',\n",
    "        'visit_concept_id'\n",
    "    ])\n",
    "    \n",
    "    visit_proj = ensure_int64(visit_proj, 'visit_concept_id')\n",
    "    \n",
    "    # Join with visit info\n",
    "    cond_joined = cond_with_ccsr.join(\n",
    "        visit_proj,\n",
    "        keys='visit_occurrence_id',\n",
    "        join_type='left outer'\n",
    "    )\n",
    "    \n",
    "    cond_joined = ensure_int64(cond_joined, 'visit_concept_id')\n",
    "    \n",
    "    # Filter to valid persons\n",
    "    valid_mask = pc.is_in(\n",
    "        cond_joined['person_id'],\n",
    "        pa.array(list(valid_person_ids), type=cond_joined['person_id'].type)\n",
    "    )\n",
    "    cond_joined = cond_joined.filter(valid_mask)\n",
    "    \n",
    "    # Parse dates\n",
    "    date_str = pc.cast(cond_joined['condition_start_date'], pa.string())\n",
    "    date_parsed = pc.strptime(date_str, format='%Y-%m-%d', unit='s')\n",
    "    cond_joined = cond_joined.append_column('condition_date_parsed', date_parsed)\n",
    "    \n",
    "    logger.info(f\"Final joined table: {cond_joined.num_rows:,} rows\")\n",
    "    \n",
    "    return cond_joined\n",
    "\n",
    "# Join conditions with CCSR\n",
    "conditions_with_ccsr = join_conditions_with_ccsr(\n",
    "    loader, \n",
    "    snomed_ccsr_chronic, \n",
    "    valid_person_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Separate Inpatient and Outpatient Encounters\n",
    "\n",
    "OMOP standard visit concept IDs:\n",
    "- **9201**: Inpatient visit\n",
    "- **9202**: Outpatient visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_ip_op(cond_table: pa.Table) -> Tuple[pa.Table, pa.Table]:\n",
    "    \"\"\"\n",
    "    Separate inpatient and outpatient encounters.\n",
    "    \"\"\"\n",
    "    logger.info(\"Separating inpatient and outpatient encounters...\")\n",
    "    \n",
    "    # Create masks for IP and OP\n",
    "    mask_ip = pc.equal(cond_table['visit_concept_id'], pa.scalar(9201, pa.int64()))\n",
    "    mask_op = pc.equal(cond_table['visit_concept_id'], pa.scalar(9202, pa.int64()))\n",
    "    \n",
    "    # Filter to IP with valid IP categories\n",
    "    ip_table = cond_table.filter(\n",
    "        pc.and_(mask_ip, pc.is_valid(cond_table['ip_cat']))\n",
    "    )\n",
    "    \n",
    "    # Filter to OP with valid OP categories\n",
    "    op_table = cond_table.filter(\n",
    "        pc.and_(mask_op, pc.is_valid(cond_table['op_cat']))\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Inpatient encounters: {ip_table.num_rows:,}\")\n",
    "    logger.info(f\"Outpatient encounters: {op_table.num_rows:,}\")\n",
    "    \n",
    "    return ip_table, op_table\n",
    "\n",
    "# Separate encounters\n",
    "ip_conditions, op_conditions = separate_ip_op(conditions_with_ccsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create CCSR Dictionaries with Earliest Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ccsr_dictionaries(ip_table: pa.Table, \n",
    "                             op_table: pa.Table,\n",
    "                             output_dir: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Create patient dictionaries by CCSR category for IP and OP separately.\n",
    "    Includes earliest occurrence dates.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating CCSR chronic condition dictionaries...\")\n",
    "    \n",
    "    # Process Inpatient\n",
    "    if ip_table.num_rows > 0:\n",
    "        grouped_ip = (\n",
    "            ip_table\n",
    "            .group_by(['ip_cat', 'person_id'])\n",
    "            .aggregate([('condition_date_parsed', 'min')])\n",
    "            .rename_columns(['ip_cat', 'person_id', 'min_date'])\n",
    "        )\n",
    "        \n",
    "        # Build dictionaries\n",
    "        ccsr_ip = defaultdict(set)\n",
    "        ccsr_ip_dates = defaultdict(dict)\n",
    "        \n",
    "        for cat, pid, date in zip(\n",
    "            grouped_ip['ip_cat'],\n",
    "            grouped_ip['person_id'],\n",
    "            timestamp_to_date_string(grouped_ip['min_date'])\n",
    "        ):\n",
    "            if cat is not None and pid is not None:\n",
    "                cat_str = cat.as_py()\n",
    "                pid_int = pid.as_py()\n",
    "                ccsr_ip[cat_str].add(pid_int)\n",
    "                ccsr_ip_dates[cat_str][pid_int] = date\n",
    "        \n",
    "        ccsr_ip = {k: sorted(list(v)) for k, v in ccsr_ip.items()}\n",
    "        logger.info(f\"Created {len(ccsr_ip)} inpatient CCSR categories\")\n",
    "    else:\n",
    "        ccsr_ip = {}\n",
    "        ccsr_ip_dates = {}\n",
    "        logger.warning(\"No inpatient encounters found\")\n",
    "    \n",
    "    # Process Outpatient\n",
    "    if op_table.num_rows > 0:\n",
    "        grouped_op = (\n",
    "            op_table\n",
    "            .group_by(['op_cat', 'person_id'])\n",
    "            .aggregate([('condition_date_parsed', 'min')])\n",
    "            .rename_columns(['op_cat', 'person_id', 'min_date'])\n",
    "        )\n",
    "        \n",
    "        # Build dictionaries\n",
    "        ccsr_op = defaultdict(set)\n",
    "        ccsr_op_dates = defaultdict(dict)\n",
    "        \n",
    "        for cat, pid, date in zip(\n",
    "            grouped_op['op_cat'],\n",
    "            grouped_op['person_id'],\n",
    "            timestamp_to_date_string(grouped_op['min_date'])\n",
    "        ):\n",
    "            if cat is not None and pid is not None:\n",
    "                cat_str = cat.as_py()\n",
    "                pid_int = pid.as_py()\n",
    "                ccsr_op[cat_str].add(pid_int)\n",
    "                ccsr_op_dates[cat_str][pid_int] = date\n",
    "        \n",
    "        ccsr_op = {k: sorted(list(v)) for k, v in ccsr_op.items()}\n",
    "        logger.info(f\"Created {len(ccsr_op)} outpatient CCSR categories\")\n",
    "    else:\n",
    "        ccsr_op = {}\n",
    "        ccsr_op_dates = {}\n",
    "        logger.warning(\"No outpatient encounters found\")\n",
    "    \n",
    "    # Save dictionaries\n",
    "    results = {\n",
    "        'patients_by_ccsr_chronic_ip': ccsr_ip,\n",
    "        'patients_by_ccsr_chronic_ip_first_date': ccsr_ip_dates,\n",
    "        'patients_by_ccsr_chronic_op': ccsr_op,\n",
    "        'patients_by_ccsr_chronic_op_first_date': ccsr_op_dates\n",
    "    }\n",
    "    \n",
    "    for name, data in results.items():\n",
    "        save_dictionary(data, f\"{name}.json\", output_dir)\n",
    "    \n",
    "    logger.info(\"CCSR chronic condition dictionaries created successfully\")\n",
    "    return results\n",
    "\n",
    "# Create dictionaries\n",
    "ccsr_results = create_ccsr_dictionaries(\n",
    "    ip_conditions,\n",
    "    op_conditions,\n",
    "    config.output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "output_files = list(config.output_dir.glob('*.json'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CCSR DICTIONARY CREATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutput directory: {config.output_dir}\")\n",
    "print(f\"\\nGenerated {len(output_files)} files:\\n\")\n",
    "\n",
    "for file in sorted(output_files):\n",
    "    size_kb = file.stat().st_size / 1024\n",
    "    print(f\"  • {file.name:<50} ({size_kb:>8.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nDictionary Structure:\")\n",
    "print(\"  - patients_by_ccsr_chronic_ip.json\")\n",
    "print(\"    Format: {ccsr_category: [patient_ids]}\")\n",
    "print(\"\\n  - patients_by_ccsr_chronic_ip_first_date.json\")\n",
    "print(\"    Format: {ccsr_category: {patient_id: first_date}}\")\n",
    "print(\"\\n  - patients_by_ccsr_chronic_op.json\")\n",
    "print(\"    Format: {ccsr_category: [patient_ids]}\")\n",
    "print(\"\\n  - patients_by_ccsr_chronic_op_first_date.json\")\n",
    "print(\"    Format: {ccsr_category: {patient_id: first_date}}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample output\n",
    "if ccsr_results['patients_by_ccsr_chronic_ip']:\n",
    "    sample_cat = list(ccsr_results['patients_by_ccsr_chronic_ip'].keys())[0]\n",
    "    sample_patients = ccsr_results['patients_by_ccsr_chronic_ip'][sample_cat]\n",
    "    print(f\"\\nExample - Inpatient '{sample_cat}':\")\n",
    "    print(f\"  Patient count: {len(sample_patients)}\")\n",
    "    print(f\"  Sample IDs: {sample_patients[:5]}\")\n",
    "\n",
    "if ccsr_results['patients_by_ccsr_chronic_op']:\n",
    "    sample_cat = list(ccsr_results['patients_by_ccsr_chronic_op'].keys())[0]\n",
    "    sample_patients = ccsr_results['patients_by_ccsr_chronic_op'][sample_cat]\n",
    "    print(f\"\\nExample - Outpatient '{sample_cat}':\")\n",
    "    print(f\"  Patient count: {len(sample_patients)}\")\n",
    "    print(f\"  Sample IDs: {sample_patients[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
