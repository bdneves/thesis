{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OMOP to MEDS ETL Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a comprehensive Extract, Transform, Load (ETL) pipeline that converts OMOP Common Data Model (CDM) format data into the Medical Event Data Standard (MEDS) format. The pipeline is designed with robust error handling, validation, and data quality checks.\n",
    "\n",
    "### What is MEDS?\n",
    "\n",
    "MEDS (Medical Event Data Standard) is a standardized format for representing longitudinal electronic health record (EHR) data. It organizes medical events as time-series data, making it ideal for:\n",
    "- Machine learning applications\n",
    "- Temporal analysis\n",
    "- Patient trajectory modeling\n",
    "- Predictive analytics\n",
    "\n",
    "### Pipeline Features\n",
    "\n",
    "- **Automated Format Detection**: Handles both Parquet and Delta Lake formats\n",
    "- **Comprehensive Validation**: Schema validation, null checks, and temporal consistency\n",
    "- **Robust Error Handling**: Graceful failure recovery with detailed logging\n",
    "- **Concept Mapping**: Automatic conversion from OMOP concept IDs to vocabulary codes\n",
    "- **Temporal Validation**: Ensures events occur after birth and in logical sequence\n",
    "- **Metadata Generation**: Creates MEDS-compliant metadata files\n",
    "\n",
    "### OMOP Tables Processed\n",
    "\n",
    "1. **person**: Patient demographics and birth information\n",
    "2. **visit_occurrence**: Healthcare visits and encounters\n",
    "3. **condition_occurrence**: Diagnoses and conditions\n",
    "4. **drug_exposure**: Medication prescriptions and administrations\n",
    "5. **procedure_occurrence**: Medical procedures\n",
    "6. **measurement**: Laboratory results and vital signs\n",
    "\n",
    "### Output Format\n",
    "\n",
    "The pipeline produces MEDS-formatted data with the following schema:\n",
    "- `PERSON_ID`: Patient identifier\n",
    "- `time`: Event timestamp (aligned with visit start time when applicable)\n",
    "- `code`: Standardized medical code (vocabulary/code format)\n",
    "- `numeric_value`: Numeric measurement value (for lab results)\n",
    "- `datetime_value`: Original event datetime\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, logging, and configuration management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField, \n",
    "    StringType, \n",
    "    ArrayType,\n",
    "    TimestampType,\n",
    "    FloatType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Configuration and Utility Functions\n",
    "\n",
    "This section defines:\n",
    "- **MEDS Schema**: The standardized schema for MEDS output\n",
    "- **ETLConfig**: Configuration dataclass for managing paths and table definitions\n",
    "- **Logging Setup**: Structured logging for monitoring pipeline execution\n",
    "- **Utility Functions**: Helper functions for validation and data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meds_schema():\n",
    "    \"\"\"Define the standardized schema for MEDS data.\n",
    "    \n",
    "    Returns:\n",
    "        StructType: PySpark schema definition for MEDS format\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"PERSON_ID\", StringType(), True),\n",
    "        StructField(\"time\", TimestampType(), True),\n",
    "        StructField(\"datetime_value\", TimestampType(), True),\n",
    "        StructField(\"code\", StringType(), True),\n",
    "        StructField(\"numeric_value\", FloatType(), True)\n",
    "    ])\n",
    "\n",
    "def setup_notebook_logging():\n",
    "    \"\"\"Configure logging for the notebook.\n",
    "    \n",
    "    Returns:\n",
    "        logging.Logger: Configured logger instance\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_notebook_logging()\n",
    "\n",
    "@dataclass\n",
    "class ETLConfig:\n",
    "    \"\"\"Configuration class for the ETL pipeline.\n",
    "    \n",
    "    Attributes:\n",
    "        base_path: Path to cleaned OMOP data\n",
    "        source_path: Path to raw OMOP source data (for concept table)\n",
    "        output_path: Base path for MEDS output\n",
    "        patient_tables: Dictionary mapping table names to required columns\n",
    "    \"\"\"\n",
    "    base_path: str\n",
    "    source_path: str\n",
    "    output_path: str\n",
    "    patient_tables: Dict[str, List[str]]\n",
    "    \n",
    "    @property\n",
    "    def unsorted_dir(self) -> str:\n",
    "        \"\"\"Directory for intermediate unsorted data.\"\"\"\n",
    "        return os.path.join(self.output_path, \"unsorted_data\")\n",
    "    \n",
    "    @property\n",
    "    def cleaned_dir(self) -> str:\n",
    "        \"\"\"Directory for final cleaned MEDS data.\"\"\"\n",
    "        return os.path.join(self.output_path, \"cleaned_data\")\n",
    "    \n",
    "    @property\n",
    "    def metadata_dir(self) -> str:\n",
    "        \"\"\"Directory for MEDS metadata files.\"\"\"\n",
    "        return os.path.join(self.output_path, \"metadata\")\n",
    "    \n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate that required paths exist and create output directories.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If required paths don't exist\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Note: In non-Databricks environments, replace dbutils calls with appropriate file system operations\n",
    "            # For local file systems, use os.path.exists() and os.makedirs()\n",
    "            \n",
    "            # Validate source paths exist\n",
    "            if not os.path.exists(self.base_path.replace('dbfs:', '/dbfs')):\n",
    "                raise ValueError(f\"Base path does not exist: {self.base_path}\")\n",
    "            \n",
    "            # Create output directories\n",
    "            for path in [self.unsorted_dir, self.cleaned_dir, self.metadata_dir]:\n",
    "                os.makedirs(path.replace('dbfs:', '/dbfs'), exist_ok=True)\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Path validation failed: {str(e)}\")\n",
    "\n",
    "def create_config() -> ETLConfig:\n",
    "    \"\"\"Create and validate ETL configuration.\n",
    "    \n",
    "    Modify the paths below to match your environment.\n",
    "    \n",
    "    Returns:\n",
    "        ETLConfig: Validated configuration object\n",
    "    \"\"\"\n",
    "    config = ETLConfig(\n",
    "        # REPLACE THESE PATHS WITH YOUR ACTUAL DATA LOCATIONS\n",
    "        base_path=\"/path/to/your/cleaned/omop/data/\",\n",
    "        source_path=\"/path/to/your/raw/omop/data/\",\n",
    "        output_path=\"/path/to/your/meds/output/\",\n",
    "        \n",
    "        # Define which columns to extract from each OMOP table\n",
    "        patient_tables={\n",
    "            \"person\": [\n",
    "                \"PERSON_ID\", \n",
    "                \"YEAR_OF_BIRTH\", \n",
    "                \"MONTH_OF_BIRTH\", \n",
    "                \"DAY_OF_BIRTH\", \n",
    "                \"BIRTH_DATETIME\"\n",
    "            ],\n",
    "            \"condition_occurrence\": [\n",
    "                \"PERSON_ID\", \n",
    "                \"CONDITION_CONCEPT_ID\", \n",
    "                \"CONDITION_START_DATETIME\", \n",
    "                \"VISIT_OCCURRENCE_ID\"\n",
    "            ],\n",
    "            \"drug_exposure\": [\n",
    "                \"PERSON_ID\", \n",
    "                \"DRUG_CONCEPT_ID\", \n",
    "                \"DRUG_EXPOSURE_START_DATETIME\", \n",
    "                \"VISIT_OCCURRENCE_ID\"\n",
    "            ],\n",
    "            \"procedure_occurrence\": [\n",
    "                \"PERSON_ID\", \n",
    "                \"PROCEDURE_CONCEPT_ID\", \n",
    "                \"PROCEDURE_DATETIME\", \n",
    "                \"VISIT_OCCURRENCE_ID\"\n",
    "            ],\n",
    "            \"visit_occurrence\": [\n",
    "                \"PERSON_ID\", \n",
    "                \"VISIT_CONCEPT_ID\", \n",
    "                \"VISIT_OCCURRENCE_ID\", \n",
    "                \"VISIT_START_DATETIME\"\n",
    "            ],\n",
    "            \"measurement\": [\n",
    "                \"PERSON_ID\", \n",
    "                \"MEASUREMENT_CONCEPT_ID\", \n",
    "                \"MEASUREMENT_DATETIME\", \n",
    "                \"VALUE_AS_NUMBER\", \n",
    "                \"VISIT_OCCURRENCE_ID\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    config.validate()\n",
    "    return config\n",
    "\n",
    "# Initialize configuration\n",
    "config = create_config()\n",
    "logger.info(\"Configuration initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Validation Functions\n",
    "\n",
    "These functions ensure data quality throughout the ETL process by:\n",
    "- Validating DataFrame schemas\n",
    "- Checking for required columns\n",
    "- Computing and logging data quality metrics\n",
    "- Detecting empty datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataframe(df: DataFrame, table_name: str, required_columns: List[str]) -> None:\n",
    "    \"\"\"Validate DataFrame schema and contents.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to validate\n",
    "        table_name: Name of the table being validated\n",
    "        required_columns: List of columns that must be present\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    # Check for required columns\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns in {table_name}: {missing_columns}\")\n",
    "    \n",
    "    # Check for empty DataFrame\n",
    "    if df.rdd.isEmpty():\n",
    "        raise ValueError(f\"Empty DataFrame for {table_name}\")\n",
    "    \n",
    "    # Log data quality metrics\n",
    "    row_count = df.count()\n",
    "    null_counts = {col: df.filter(F.col(col).isNull()).count() for col in required_columns}\n",
    "    \n",
    "    logger.info(f\"Table {table_name} statistics:\")\n",
    "    logger.info(f\"  Total rows: {row_count:,}\")\n",
    "    logger.info(f\"  Null counts: {null_counts}\")\n",
    "\n",
    "def verify_path_exists(spark: SparkSession, path: str) -> bool:\n",
    "    \"\"\"Verify if a path exists in the file system.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        path: Path to verify\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if path exists, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For Databricks: use dbutils.fs.ls(path)\n",
    "        # For standard Spark: use os.path.exists()\n",
    "        return os.path.exists(path.replace('dbfs:', '/dbfs'))\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Concept Mapping Functions\n",
    "\n",
    "OMOP uses concept IDs to represent medical terminology. These functions:\n",
    "- Load the OMOP concept vocabulary\n",
    "- Create mappings from concept IDs to standardized codes (e.g., SNOMED/123456)\n",
    "- Enable conversion to MEDS format with human-readable codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concept_maps(spark: SparkSession, config: ETLConfig) -> Dict[str, str]:\n",
    "    \"\"\"Create concept ID to code mappings with error handling.\n",
    "    \n",
    "    This function reads the OMOP concept table and creates a dictionary\n",
    "    mapping concept IDs to vocabulary-prefixed codes (e.g., \"SNOMED/12345\").\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        config: ETL configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary mapping concept IDs to codes\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If concept table cannot be read\n",
    "    \"\"\"\n",
    "    try:\n",
    "        concept_df = read_table_safely(\n",
    "            spark,\n",
    "            \"concept\", \n",
    "            config.source_path,\n",
    "            [\"CONCEPT_ID\", \"VOCABULARY_ID\", \"CONCEPT_CODE\"]\n",
    "        )\n",
    "        \n",
    "        if concept_df is None:\n",
    "            raise ValueError(\"Failed to read concept table\")\n",
    "\n",
    "        # Create codes in format \"VOCABULARY/CODE\"\n",
    "        concept_df = concept_df.withColumn(\n",
    "            \"code\", \n",
    "            F.concat_ws(\"/\", F.col(\"VOCABULARY_ID\"), F.col(\"CONCEPT_CODE\"))\n",
    "        )\n",
    "        \n",
    "        # Convert to dictionary efficiently using broadcast\n",
    "        concept_map = dict(\n",
    "            concept_df.select(\"CONCEPT_ID\", \"code\")\n",
    "                     .rdd\n",
    "                     .collectAsMap()\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Created concept map with {len(concept_map):,} entries\")\n",
    "        return concept_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating concept maps: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metadata Generation\n",
    "\n",
    "Generate MEDS-compliant metadata files that document:\n",
    "- Dataset name and version\n",
    "- ETL pipeline version\n",
    "- MEDS format version\n",
    "- Creation timestamp\n",
    "\n",
    "This metadata is essential for data provenance and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata(spark: SparkSession, config: ETLConfig):\n",
    "    \"\"\"Generate metadata files for the MEDS dataset.\n",
    "    \n",
    "    Creates a JSON file with dataset information following MEDS standards.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        config: ETL configuration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Generating metadata...\")\n",
    "        \n",
    "        # Create dataset metadata\n",
    "        dataset_metadata = {\n",
    "            \"dataset_name\": \"MEDS Dataset\",\n",
    "            \"dataset_version\": \"1.0\",\n",
    "            \"etl_name\": \"omop_to_meds_etl\",\n",
    "            \"etl_version\": \"1.0\",\n",
    "            \"meds_version\": \"1.0\",\n",
    "            \"created_at\": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())\n",
    "        }\n",
    "        \n",
    "        # Write metadata file\n",
    "        metadata_path = os.path.join(config.metadata_dir, \"dataset.json\")\n",
    "        with open(metadata_path.replace('dbfs:', '/dbfs'), 'w') as f:\n",
    "            json.dump(dataset_metadata, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Metadata written to {metadata_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating metadata: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Reading Functions\n",
    "\n",
    "These functions provide robust data reading capabilities:\n",
    "- **Format Detection**: Automatically detect Parquet or Delta Lake formats\n",
    "- **Safe Reading**: Handle missing files and invalid formats gracefully\n",
    "- **Column Validation**: Ensure required columns are present\n",
    "- **Retry Logic**: Attempt multiple read strategies before failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_table_format(spark: SparkSession, path: str) -> str:\n",
    "    \"\"\"Detect the format of data at the given path.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        path: Path to the data\n",
    "        \n",
    "    Returns:\n",
    "        str: Format type ('delta' or 'parquet')\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If format cannot be detected\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to read as Delta first\n",
    "        spark.read.format(\"delta\").load(path).limit(1).count()\n",
    "        return \"delta\"\n",
    "    except Exception as e:\n",
    "        if \"Incompatible format\" in str(e) or \"not a Delta table\" in str(e):\n",
    "            # Try to read as Parquet\n",
    "            try:\n",
    "                spark.read.parquet(path).limit(1).count()\n",
    "                return \"parquet\"\n",
    "            except:\n",
    "                pass\n",
    "        raise ValueError(f\"Unable to detect readable format for {path}\")\n",
    "\n",
    "def read_table_safely(\n",
    "    spark: SparkSession,\n",
    "    table_name: str,\n",
    "    base_path: str,\n",
    "    required_columns: List[str]\n",
    ") -> Optional[DataFrame]:\n",
    "    \"\"\"Safely read a table with validation and error handling.\n",
    "    \n",
    "    Attempts to read data in multiple formats (Parquet, Delta) and\n",
    "    validates that all required columns are present.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        table_name: Name of the table to read\n",
    "        base_path: Base directory containing tables\n",
    "        required_columns: List of columns that must be present\n",
    "    \n",
    "    Returns:\n",
    "        Optional[DataFrame]: DataFrame with required columns, or None if read fails\n",
    "    \"\"\"\n",
    "    table_path = os.path.join(base_path, table_name)\n",
    "    \n",
    "    if not verify_path_exists(spark, table_path):\n",
    "        logger.warning(f\"Table path does not exist: {table_path}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try reading as parquet first\n",
    "        df = spark.read.parquet(table_path)\n",
    "        logger.info(f\"Successfully read {table_name} as parquet\")\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            # Try reading as delta format\n",
    "            df = spark.read.format(\"delta\").load(table_path)\n",
    "            logger.info(f\"Successfully read {table_name} as delta\")\n",
    "        except Exception as e2:\n",
    "            logger.error(f\"Failed to read {table_name} in both formats\")\n",
    "            logger.error(f\"  Parquet error: {str(e1)}\")\n",
    "            logger.error(f\"  Delta error: {str(e2)}\")\n",
    "            return None\n",
    "    \n",
    "    # Validate columns\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        logger.error(f\"Missing required columns in {table_name}: {missing_columns}\")\n",
    "        return None\n",
    "        \n",
    "    return df.select(*required_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Transformation Functions\n",
    "\n",
    "These functions handle the core transformation logic:\n",
    "\n",
    "### Person Table Processing\n",
    "- Creates birth datetime from year/month/day components\n",
    "- Assigns birth event code (SNOMED/184099003 = \"Live birth\")\n",
    "\n",
    "### Clinical Table Processing\n",
    "- Maps OMOP concept IDs to vocabulary codes\n",
    "- Aligns event times with visit start times\n",
    "- Preserves original datetime values\n",
    "- Handles numeric measurements (lab values)\n",
    "\n",
    "### Temporal Alignment\n",
    "All clinical events are aligned to their associated visit's start time, which provides:\n",
    "- Consistent temporal ordering\n",
    "- Simplified sequential modeling\n",
    "- Reduced temporal granularity for privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_datetime(df: DataFrame, time_columns: List[str]) -> DataFrame:\n",
    "    \"\"\"Cast time columns to datetime with validation.\n",
    "    \n",
    "    Attempts to cast multiple columns to timestamp, using coalesce\n",
    "    to handle nulls gracefully.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        time_columns: List of column names to try casting\n",
    "        \n",
    "    Returns:\n",
    "        Column expression with datetime value\n",
    "    \"\"\"\n",
    "    valid_time_columns = [col_name for col_name in time_columns if col_name in df.columns]\n",
    "    \n",
    "    if not valid_time_columns:\n",
    "        return F.lit(None).cast(\"timestamp\")\n",
    "    \n",
    "    time_col = None\n",
    "    for col_name in valid_time_columns:\n",
    "        datetime_col = F.col(col_name).cast(\"timestamp\")\n",
    "        if time_col is None:\n",
    "            time_col = datetime_col\n",
    "        else:\n",
    "            time_col = F.coalesce(time_col, datetime_col)\n",
    "    \n",
    "    return time_col\n",
    "\n",
    "def process_person_table(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Process person table to create birth events.\n",
    "    \n",
    "    Transforms person demographics into MEDS format with birth events.\n",
    "    Birth date is constructed from year, month, and day components.\n",
    "    \n",
    "    Args:\n",
    "        df: Person DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame in MEDS format with birth events\n",
    "    \"\"\"\n",
    "    # Filter records with valid birth year\n",
    "    df = df.filter(F.col(\"YEAR_OF_BIRTH\").isNotNull())\n",
    "    \n",
    "    # Create birth datetime from components\n",
    "    df = df.withColumn(\n",
    "        \"time\",\n",
    "        F.to_timestamp(\n",
    "            F.concat_ws(\n",
    "                '-',\n",
    "                F.col(\"YEAR_OF_BIRTH\"),\n",
    "                F.lpad(F.coalesce(F.col(\"MONTH_OF_BIRTH\"), F.lit(1)), 2, '0'),\n",
    "                F.lpad(F.coalesce(F.col(\"DAY_OF_BIRTH\"), F.lit(1)), 2, '0')\n",
    "            ),\n",
    "            \"yyyy-MM-dd\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Set datetime_value from birth datetime if available\n",
    "    df = df.withColumn(\"datetime_value\", F.col(\"BIRTH_DATETIME\").cast(\"timestamp\"))\n",
    "    \n",
    "    # Assign birth event code (SNOMED: Live birth)\n",
    "    df = df.withColumn(\"code\", F.lit(\"SNOMED/184099003\"))\n",
    "    \n",
    "    # No numeric value for birth events\n",
    "    df = df.withColumn(\"numeric_value\", F.lit(None).cast(\"float\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_clinical_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    visit_occurrence_df: DataFrame,\n",
    "    concept_id_map: Dict[str, str]\n",
    ") -> DataFrame:\n",
    "    \"\"\"Process clinical tables (conditions, drugs, procedures, measurements).\n",
    "    \n",
    "    Transforms clinical event tables into MEDS format by:\n",
    "    1. Joining with visit occurrence to get visit start times\n",
    "    2. Mapping concept IDs to vocabulary codes\n",
    "    3. Preserving original datetime values\n",
    "    4. Handling numeric measurements\n",
    "    \n",
    "    Args:\n",
    "        df: Input clinical DataFrame\n",
    "        table_name: Name of the table being processed\n",
    "        visit_occurrence_df: Visit occurrence DataFrame for time alignment\n",
    "        concept_id_map: Dictionary mapping concept IDs to codes\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame in MEDS format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle visit occurrence table differently\n",
    "    if table_name == \"visit_occurrence\":\n",
    "        time_col = F.col(\"VISIT_START_DATETIME\").cast(\"timestamp\")\n",
    "        datetime_value_col = time_col\n",
    "    else:\n",
    "        # Join with visit occurrence to align event times\n",
    "        if \"VISIT_OCCURRENCE_ID\" in df.columns:\n",
    "            visit_df = visit_occurrence_df.withColumnRenamed(\n",
    "                \"VISIT_START_DATETIME\", \n",
    "                \"visit_start_time\"\n",
    "            )\n",
    "            df = df.join(\n",
    "                visit_df,\n",
    "                on=\"VISIT_OCCURRENCE_ID\",\n",
    "                how=\"left\"\n",
    "            )\n",
    "            time_col = F.col(\"visit_start_time\").cast(\"timestamp\")\n",
    "        else:\n",
    "            time_col = F.lit(None).cast(\"timestamp\")\n",
    "        \n",
    "        # Preserve original datetime for clinical events\n",
    "        datetime_columns = [col for col in df.columns if \"_DATETIME\" in col]\n",
    "        datetime_value_col = cast_to_datetime(df, datetime_columns)\n",
    "    \n",
    "    # Add time columns\n",
    "    df = df.withColumn(\"time\", time_col)\n",
    "    df = df.withColumn(\"datetime_value\", datetime_value_col)\n",
    "    \n",
    "    # Map concept IDs to vocabulary codes\n",
    "    concept_col = next(col for col in df.columns if \"CONCEPT_ID\" in col)\n",
    "    concept_map_df = spark.createDataFrame(\n",
    "        [(k, v) for k, v in concept_id_map.items()],\n",
    "        [\"concept_id\", \"code\"]\n",
    "    )\n",
    "    \n",
    "    df = df.join(\n",
    "        F.broadcast(concept_map_df),\n",
    "        df[concept_col] == concept_map_df[\"concept_id\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Handle numeric values (measurements have VALUE_AS_NUMBER)\n",
    "    if table_name == \"measurement\":\n",
    "        df = df.withColumn(\n",
    "            \"numeric_value\",\n",
    "            F.col(\"VALUE_AS_NUMBER\").cast(\"float\")\n",
    "        )\n",
    "    else:\n",
    "        df = df.withColumn(\n",
    "            \"numeric_value\",\n",
    "            F.lit(None).cast(\"float\")\n",
    "        )\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    if \"visit_start_time\" in df.columns:\n",
    "        df = df.drop(\"visit_start_time\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Table Processing Pipeline\n",
    "\n",
    "This function orchestrates the transformation of individual OMOP tables into MEDS format.\n",
    "\n",
    "### Processing Steps:\n",
    "1. **Read**: Load table with required columns\n",
    "2. **Transform**: Apply table-specific transformation logic\n",
    "3. **Validate**: Check data quality and completeness\n",
    "4. **Write**: Save to intermediate storage with retry logic\n",
    "\n",
    "### Error Handling:\n",
    "- Graceful failure: Individual table failures don't stop the pipeline\n",
    "- Retry logic: Up to 3 attempts for write operations\n",
    "- Detailed logging: All errors are logged for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(\n",
    "    spark: SparkSession,\n",
    "    table_name: str,\n",
    "    config: ETLConfig,\n",
    "    concept_id_map: Dict[str, str],\n",
    "    visit_occurrence_df: DataFrame\n",
    ") -> None:\n",
    "    \"\"\"Process a patient-related table with enhanced error handling.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        table_name: Name of the table to process\n",
    "        config: ETL configuration\n",
    "        concept_id_map: Mapping of concept IDs to codes\n",
    "        visit_occurrence_df: Visit occurrence DataFrame for joins\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processing {table_name}...\")\n",
    "        \n",
    "        # Read table with safety checks\n",
    "        required_columns = config.patient_tables[table_name]\n",
    "        df = read_table_safely(spark, table_name, config.base_path, required_columns)\n",
    "        \n",
    "        if df is None:\n",
    "            logger.error(f\"Skipping {table_name} due to read/validation errors\")\n",
    "            return\n",
    "            \n",
    "        # Process based on table type\n",
    "        if table_name == \"person\":\n",
    "            df = process_person_table(df)\n",
    "        else:\n",
    "            df = process_clinical_table(df, table_name, visit_occurrence_df, concept_id_map)\n",
    "        \n",
    "        # Prepare output path\n",
    "        output_path = os.path.join(config.unsorted_dir, f\"{table_name}.parquet\")\n",
    "        \n",
    "        # Write with retry logic\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                df.write.mode(\"overwrite\").parquet(output_path, compression='snappy')\n",
    "                logger.info(f\"Successfully wrote {table_name} to {output_path}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                logger.warning(f\"Write attempt {attempt + 1} failed, retrying...\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "        logger.info(f\"Successfully processed {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {table_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Cleaning and Validation\n",
    "\n",
    "After initial transformation, this function applies comprehensive cleaning:\n",
    "\n",
    "### Cleaning Operations:\n",
    "1. **Person Validation**: Remove events for non-existent patients\n",
    "2. **Temporal Validation**: Ensure events occur after birth\n",
    "3. **Datetime Consistency**: Fix datetime values that violate temporal logic\n",
    "4. **Quality Metrics**: Calculate and log retention rates\n",
    "\n",
    "### Temporal Validation Rules:\n",
    "- Events must occur after patient birth\n",
    "- `datetime_value` must not be before `time`\n",
    "- Events should maintain logical temporal ordering\n",
    "\n",
    "These validations ensure the data is suitable for time-series modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_person_data(spark: SparkSession, config: ETLConfig) -> DataFrame:\n",
    "    \"\"\"Prepare person data for temporal validation.\n",
    "    \n",
    "    Creates a DataFrame with person IDs and birth dates for filtering.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        config: ETL configuration\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with person IDs and birth dates\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If person data cannot be read\n",
    "    \"\"\"\n",
    "    person_df = read_table_safely(\n",
    "        spark,\n",
    "        \"person\",\n",
    "        config.base_path,\n",
    "        [\"PERSON_ID\", \"YEAR_OF_BIRTH\", \"MONTH_OF_BIRTH\", \"DAY_OF_BIRTH\"]\n",
    "    )\n",
    "    \n",
    "    if person_df is None:\n",
    "        raise ValueError(\"Failed to read person data\")\n",
    "    \n",
    "    return person_df.withColumn(\n",
    "        \"birthdate\",\n",
    "        F.to_timestamp(\n",
    "            F.concat_ws(\n",
    "                '-',\n",
    "                F.col(\"YEAR_OF_BIRTH\"),\n",
    "                F.lpad(F.coalesce(F.col(\"MONTH_OF_BIRTH\"), F.lit(1)), 2, '0'),\n",
    "                F.lpad(F.coalesce(F.col(\"DAY_OF_BIRTH\"), F.lit(1)), 2, '0')\n",
    "            ),\n",
    "            \"yyyy-MM-dd\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def apply_temporal_validations(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Apply temporal validations to MEDS data.\n",
    "    \n",
    "    Ensures temporal consistency by:\n",
    "    1. Filtering events that occur before birth\n",
    "    2. Correcting datetime_value when it precedes time\n",
    "    3. Maintaining event ordering within patient timelines\n",
    "    \n",
    "    Args:\n",
    "        df: Input MEDS DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Temporally validated DataFrame\n",
    "    \"\"\"\n",
    "    # Filter events before birth\n",
    "    df = df.filter(F.col(\"time\") >= F.col(\"birthdate\"))\n",
    "    \n",
    "    # Ensure datetime_value is not before time\n",
    "    df = df.withColumn(\n",
    "        \"datetime_value\",\n",
    "        F.when(\n",
    "            F.col(\"datetime_value\") < F.col(\"time\"), \n",
    "            F.col(\"time\")\n",
    "        ).otherwise(F.col(\"datetime_value\"))\n",
    "    )\n",
    "    \n",
    "    # Handle temporal ordering between events\n",
    "    window_spec = Window.partitionBy(\"PERSON_ID\").orderBy(\"time\")\n",
    "    df = df.withColumn(\"next_event_time\", F.lead(\"time\").over(window_spec))\n",
    "    \n",
    "    # Correct datetime_value if it exceeds next event time\n",
    "    df = df.withColumn(\n",
    "        \"datetime_value\",\n",
    "        F.when(\n",
    "            (F.col(\"next_event_time\").isNotNull()) & \n",
    "            (F.col(\"datetime_value\") > F.col(\"next_event_time\")),\n",
    "            F.col(\"time\")\n",
    "        ).otherwise(F.col(\"datetime_value\"))\n",
    "    )\n",
    "    \n",
    "    # Remove temporary columns\n",
    "    return df.drop(\"next_event_time\", \"birthdate\")\n",
    "\n",
    "def clean_meds_data(spark: SparkSession, config: ETLConfig) -> int:\n",
    "    \"\"\"Clean MEDS data with comprehensive validation.\n",
    "    \n",
    "    Final cleaning step that:\n",
    "    1. Loads all transformed data\n",
    "    2. Joins with person data to filter valid patients\n",
    "    3. Applies temporal validations\n",
    "    4. Writes final cleaned dataset\n",
    "    5. Validates written data\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        config: ETL configuration\n",
    "        \n",
    "    Returns:\n",
    "        int: Final record count\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If cleaning fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting MEDS data cleaning...\")\n",
    "        \n",
    "        # Load unsorted data\n",
    "        unsorted_path = config.unsorted_dir\n",
    "        if not verify_path_exists(spark, unsorted_path):\n",
    "            raise ValueError(f\"Unsorted data directory does not exist: {unsorted_path}\")\n",
    "        \n",
    "        # Read with explicit schema\n",
    "        schema = get_meds_schema()\n",
    "        meds_df = spark.read.schema(schema).parquet(unsorted_path)\n",
    "        \n",
    "        initial_count = meds_df.count()\n",
    "        logger.info(f\"Initial event count: {initial_count:,}\")\n",
    "        \n",
    "        # Load person data for validation\n",
    "        person_df = prepare_person_data(spark, config)\n",
    "        \n",
    "        # Join with person to keep only valid patients\n",
    "        meds_df = (meds_df.join(\n",
    "            person_df,\n",
    "            meds_df.PERSON_ID == person_df.PERSON_ID,\n",
    "            \"inner\"\n",
    "        ).drop(person_df.PERSON_ID))\n",
    "        \n",
    "        # Apply temporal validations\n",
    "        meds_df = apply_temporal_validations(meds_df)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        final_count = meds_df.count()\n",
    "        records_removed = initial_count - final_count\n",
    "        retention_rate = (final_count / initial_count * 100) if initial_count > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Records removed: {records_removed:,}\")\n",
    "        logger.info(f\"Retention rate: {retention_rate:.2f}%\")\n",
    "        \n",
    "        # Write cleaned data with retry logic\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                meds_df.write.mode(\"overwrite\").parquet(\n",
    "                    config.cleaned_dir,\n",
    "                    compression='snappy'\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                logger.warning(f\"Write attempt {attempt + 1} failed, retrying...\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "        logger.info(f\"Successfully wrote {final_count:,} records to {config.cleaned_dir}\")\n",
    "        \n",
    "        # Validate written data\n",
    "        validation_df = spark.read.schema(schema).parquet(config.cleaned_dir)\n",
    "        validation_count = validation_df.count()\n",
    "        \n",
    "        if validation_count != final_count:\n",
    "            logger.error(\n",
    "                f\"Validation failed: Written records ({validation_count:,}) != \"\n",
    "                f\"Expected records ({final_count:,})\"\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"Data validation successful\")\n",
    "        \n",
    "        return final_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during MEDS data cleaning: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Main Pipeline Execution\n",
    "\n",
    "This is the main orchestration function that executes the complete ETL pipeline.\n",
    "\n",
    "### Execution Flow:\n",
    "\n",
    "1. **Initialization**\n",
    "   - Load configuration\n",
    "   - Validate paths\n",
    "   - Create output directories\n",
    "\n",
    "2. **Setup Phase**\n",
    "   - Create concept ID mappings\n",
    "   - Load and cache visit occurrence data\n",
    "\n",
    "3. **Transformation Phase**\n",
    "   - Process each OMOP table sequentially\n",
    "   - Track successful and failed tables\n",
    "\n",
    "4. **Cleaning Phase**\n",
    "   - Consolidate all transformed data\n",
    "   - Apply validation rules\n",
    "   - Write final output\n",
    "\n",
    "5. **Finalization**\n",
    "   - Generate metadata\n",
    "   - Log statistics\n",
    "   - Clean up resources\n",
    "\n",
    "### Error Handling:\n",
    "- Individual table failures don't stop the pipeline\n",
    "- Failed tables are logged for review\n",
    "- Pipeline requires at least one successful table to proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function with comprehensive error handling.\n",
    "    \n",
    "    Orchestrates the complete OMOP to MEDS ETL pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"Starting OMOP to MEDS ETL Pipeline\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        spark = SparkSession.builder.appName(\"OMOP_to_MEDS_ETL\").getOrCreate()\n",
    "        \n",
    "        # Initialize and validate config\n",
    "        config = create_config()\n",
    "        \n",
    "        # Verify all required paths exist\n",
    "        required_paths = [config.base_path, config.source_path]\n",
    "        for path in required_paths:\n",
    "            if not verify_path_exists(spark, path):\n",
    "                raise ValueError(f\"Required path does not exist: {path}\")\n",
    "        \n",
    "        logger.info(\"Configuration validated successfully\")\n",
    "        \n",
    "        # Phase 1: Create concept mappings\n",
    "        logger.info(\"\\n\" + \"-\" * 80)\n",
    "        logger.info(\"Phase 1: Creating concept mappings\")\n",
    "        logger.info(\"-\" * 80)\n",
    "        concept_id_map = create_concept_maps(spark, config)\n",
    "        \n",
    "        # Phase 2: Load and cache visit occurrence\n",
    "        logger.info(\"\\n\" + \"-\" * 80)\n",
    "        logger.info(\"Phase 2: Loading visit occurrence data\")\n",
    "        logger.info(\"-\" * 80)\n",
    "        visit_occurrence_df = read_table_safely(\n",
    "            spark,\n",
    "            \"visit_occurrence\",\n",
    "            config.base_path,\n",
    "            [\"VISIT_OCCURRENCE_ID\", \"VISIT_START_DATETIME\"]\n",
    "        )\n",
    "        \n",
    "        if visit_occurrence_df is None:\n",
    "            raise ValueError(\"Failed to read visit_occurrence table - cannot proceed\")\n",
    "            \n",
    "        visit_occurrence_df = visit_occurrence_df.cache()\n",
    "        logger.info(f\"Visit occurrence cached: {visit_occurrence_df.count():,} records\")\n",
    "        \n",
    "        # Phase 3: Process all tables\n",
    "        logger.info(\"\\n\" + \"-\" * 80)\n",
    "        logger.info(\"Phase 3: Processing OMOP tables\")\n",
    "        logger.info(\"-\" * 80)\n",
    "        \n",
    "        successful_tables = []\n",
    "        failed_tables = []\n",
    "        \n",
    "        for table_name in config.patient_tables.keys():\n",
    "            try:\n",
    "                process_table(\n",
    "                    spark,\n",
    "                    table_name,\n",
    "                    config,\n",
    "                    concept_id_map,\n",
    "                    visit_occurrence_df\n",
    "                )\n",
    "                successful_tables.append(table_name)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process {table_name}: {str(e)}\")\n",
    "                failed_tables.append(table_name)\n",
    "                continue\n",
    "        \n",
    "        if not successful_tables:\n",
    "            raise ValueError(\"No tables were processed successfully\")\n",
    "        \n",
    "        logger.info(f\"\\nSuccessfully processed: {len(successful_tables)} tables\")\n",
    "        logger.info(f\"Failed to process: {len(failed_tables)} tables\")\n",
    "        \n",
    "        # Phase 4: Clean and validate data\n",
    "        logger.info(\"\\n\" + \"-\" * 80)\n",
    "        logger.info(\"Phase 4: Cleaning and validating MEDS data\")\n",
    "        logger.info(\"-\" * 80)\n",
    "        final_count = clean_meds_data(spark, config)\n",
    "        \n",
    "        # Phase 5: Generate metadata\n",
    "        logger.info(\"\\n\" + \"-\" * 80)\n",
    "        logger.info(\"Phase 5: Generating metadata\")\n",
    "        logger.info(\"-\" * 80)\n",
    "        generate_metadata(spark, config)\n",
    "        \n",
    "        # Clean up\n",
    "        visit_occurrence_df.unpersist()\n",
    "        \n",
    "        # Final statistics\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"Pipeline Completed Successfully!\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(f\"Duration: {duration:.2f} seconds\")\n",
    "        logger.info(f\"Successfully processed tables: {successful_tables}\")\n",
    "        if failed_tables:\n",
    "            logger.warning(f\"Failed tables: {failed_tables}\")\n",
    "        logger.info(f\"Final event count: {final_count:,}\")\n",
    "        logger.info(f\"Output location: {config.cleaned_dir}\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"\\n\" + \"=\" * 80)\n",
    "        logger.error(\"Pipeline Failed!\")\n",
    "        logger.error(\"=\" * 80)\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        logger.error(\"=\" * 80)\n",
    "        raise\n",
    "    finally:\n",
    "        logger.info(\"Pipeline execution finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Execute Pipeline\n",
    "\n",
    "Run the complete ETL pipeline. Monitor the logs for progress and any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Verify Output (Optional)\n",
    "\n",
    "After the pipeline completes, you can verify the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to verify output\n",
    "\n",
    "# # Load the cleaned MEDS data\n",
    "# spark = SparkSession.builder.appName(\"MEDS_Verification\").getOrCreate()\n",
    "# config = create_config()\n",
    "\n",
    "# meds_df = spark.read.parquet(config.cleaned_dir)\n",
    "\n",
    "# # Display basic statistics\n",
    "# print(\"\\nMEDS Data Statistics:\")\n",
    "# print(f\"Total events: {meds_df.count():,}\")\n",
    "# print(f\"Unique patients: {meds_df.select('PERSON_ID').distinct().count():,}\")\n",
    "# print(f\"Unique codes: {meds_df.select('code').distinct().count():,}\")\n",
    "\n",
    "# # Show sample records\n",
    "# print(\"\\nSample records:\")\n",
    "# meds_df.show(5, truncate=False)\n",
    "\n",
    "# # Check schema\n",
    "# print(\"\\nSchema:\")\n",
    "# meds_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes and Best Practices\n",
    "\n",
    "### Data Quality Considerations\n",
    "\n",
    "**What the Pipeline Handles:**\n",
    "- Temporal alignment with visit start times\n",
    "- Birth date validation\n",
    "- Concept ID to code mapping\n",
    "- Null value handling\n",
    "- Format detection (Parquet/Delta)\n",
    "\n",
    "**What You Should Review:**\n",
    "- Failed table processing (check logs)\n",
    "- Data retention rates (should be >95%)\n",
    "- Missing concept mappings\n",
    "- Unusual temporal patterns\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "1. **Caching**: Visit occurrence data is cached since it's used for all clinical tables\n",
    "2. **Broadcasting**: Concept maps use broadcast joins for efficiency\n",
    "3. **Compression**: Output uses Snappy compression for balance of speed and size\n",
    "4. **Partitioning**: Consider repartitioning large datasets before final write\n",
    "\n",
    "### Customization Points\n",
    "\n",
    "**Adding New Tables:**\n",
    "1. Add table name and columns to `patient_tables` in `ETLConfig`\n",
    "2. Update `process_clinical_table` if special handling is needed\n",
    "3. Ensure concept ID column follows naming convention\n",
    "\n",
    "**Modifying Time Alignment:**\n",
    "- Current: All events aligned to visit start time\n",
    "- Alternative: Use original event datetime (modify `process_clinical_table`)\n",
    "\n",
    "**Custom Validation Rules:**\n",
    "- Add to `apply_temporal_validations` function\n",
    "- Consider domain-specific constraints (e.g., age limits)\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "1. **\"Table path does not exist\"**\n",
    "   - Verify paths in configuration\n",
    "   - Check file permissions\n",
    "   - Ensure OMOP data is in expected format\n",
    "\n",
    "2. **\"Missing required columns\"**\n",
    "   - Verify OMOP schema matches expected structure\n",
    "   - Update `patient_tables` configuration\n",
    "   - Check for case sensitivity in column names\n",
    "\n",
    "3. **Low retention rate (<90%)**\n",
    "   - Review temporal validation logic\n",
    "   - Check for invalid birth dates\n",
    "   - Verify person table completeness\n",
    "\n",
    "4. **Out of memory errors**\n",
    "   - Increase Spark executor memory\n",
    "   - Process tables in smaller batches\n",
    "   - Add intermediate caching/checkpointing\n",
    "\n",
    "### Environment Considerations\n",
    "\n",
    "**Databricks:**\n",
    "- Uses `dbutils` for file operations\n",
    "- DBFS paths prefixed with `dbfs:/`\n",
    "- Delta Lake format supported natively\n",
    "\n",
    "**Standard Spark:**\n",
    "- Replace `dbutils` calls with OS operations\n",
    "- Use HDFS or local file paths\n",
    "- May need Delta Lake package added\n",
    "\n",
    "**Local Development:**\n",
    "- Use small sample datasets\n",
    "- Set Spark to local mode\n",
    "- Consider using Parquet instead of Delta\n",
    "\n",
    "---\n",
    "\n",
    "## Output Format\n",
    "\n",
    "### MEDS Schema\n",
    "\n",
    "```\n",
    "PERSON_ID: string - Patient identifier\n",
    "time: timestamp - Event time (aligned to visit start)\n",
    "datetime_value: timestamp - Original event datetime\n",
    "code: string - Medical code (format: VOCABULARY/CODE)\n",
    "numeric_value: float - Numeric measurement value (null for non-measurements)\n",
    "```\n",
    "\n",
    "### Example Records\n",
    "\n",
    "```\n",
    "PERSON_ID | time                | datetime_value      | code                | numeric_value\n",
    "----------|--------------------|--------------------|---------------------|---------------\n",
    "12345     | 1980-06-15 00:00:00| 1980-06-15 08:30:00| SNOMED/184099003   | null\n",
    "12345     | 2020-01-10 09:00:00| 2020-01-10 09:15:00| SNOMED/38341003    | null\n",
    "12345     | 2020-01-10 09:00:00| 2020-01-10 10:30:00| LOINC/2093-3       | 7.2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this ETL pipeline in your research, please cite:\n",
    "\n",
    "```\n",
    "[Your Name]. (2025). OMOP to MEDS ETL Pipeline.\n",
    "GitHub: [Your Repository URL]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## License\n",
    "\n",
    "Specify your license here (e.g., MIT, Apache 2.0, etc.)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
